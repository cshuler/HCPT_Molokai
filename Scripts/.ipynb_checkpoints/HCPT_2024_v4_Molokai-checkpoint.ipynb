{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hawaii Cesspool Hazard Assessment & Prioritization Tool:\n",
    "# Molokai Version 3\n",
    "\n",
    "\n",
    "## ................................................ Code Notebook ................................................\n",
    "*September 2024*\n",
    "\n",
    "**Prepared For:** \n",
    "NOAA Coastal Zone Management Progam\n",
    "\n",
    "**Principal Investigator and Primary Software Developer** \n",
    "- Christopher Shuler Ph.D., Shuler Hydrologic LLC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlights for Version 3\n",
    "- Updated using existing methodology to include the island of Molokai\n",
    "- Using a digital ocean space cloud storage bucket to reference large files for the code, so as not to keep them in the repo\n",
    "\n",
    "\n",
    "### Methods: Input datasets\n",
    "\n",
    "#### Physical drivers and risk factors\n",
    "- These are factors that physically control the transport, attenuation, or otherwise affect the level of impact an OSDS at a given location will have on the environment and water quality\n",
    "\n",
    "- Methodology for Molokai remained consistant with the 2021/2022 version for all datasets. Where congruent datasets did not exist before (groundwater flowpaths, ecosystem factors, etc...) we developed these datasets through the assistance of our partners and are highly appreciative of their assistance in bringing Molokai into the HCPT \n",
    "- The impact of an individual OSDS depends on an array of factors regarding the substrate within which it is installed, its location and proximity to sensitive areas, the ultimate discharge point of its effluent and the conservative constituents therein, and the cumulative impacts of other nearby OSDS, which together may more quickly overwhelm the adsorption and contaminant attenuation capacity of the subsurface than an individual system would. \n",
    "- We considered all numerically or categorically quantifiable factors relating to variability in OSDS impacts, for which we were able to find statewide datasets, we call these \"Risk Factors\". \n",
    "- While some of the risk factor datasets had missing values we generally only included datasets for which we had a minimum of 90% coverage of the OSDS in the state. \n",
    "- For each risk factor we defined a scaler relationship or score between the variable and the spatial location of every individual OSDS unit which was then weighted, and averaged with the scores of all other risk factors to produce a single final prioritization ranking \n",
    "- for each individual OSDS in our state-wide inventory, we calculated a numeric value based on the relationship between the location of the OSDS unit and the spatial distribution of the risk factor so that every OSDS unit is assigned a score for every risk factor. \n",
    "\n",
    "\n",
    "#### Value- based risk factors\n",
    "While the physical factors control the level of impact, we have also included value-based factors that quantify the spatial distribution of human and environmental value within the areas calculated to be affected by the discharge of cesspool effluent through the use of groundwater flow modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up dependencies and local functions\n",
    "%run functions_v2.py\n",
    "from IPython.display import Image\n",
    "\n",
    "# relative paths   KEEPING THESE FOR A MINUTE IN CASE THE BELOW SCREWS THINGS UP\n",
    "#homedir = os.getcwd()\n",
    "#if not os.path.exists(os.path.join(homedir,'tempspace')): os.makedirs(os.path.join(homedir,'tempspace'))  \n",
    "#tempspace = os.path.join(homedir, \"tempspace\")\n",
    "#if not os.path.exists(os.path.join(\".\", 'Outputs/intermidiate_DataFrames')): os.makedirs(os.path.join(\".\", 'Outputs/intermidiate_DataFrames'))  \n",
    "#intermidiate_DataFramesPath = os.path.join(\".\", 'Outputs/intermidiate_DataFrames')\n",
    "\n",
    "homedir = os.getcwd()\n",
    "tempspace = os.path.abspath(os.path.join(homedir, \"tempspace\"))\n",
    "os.makedirs(tempspace, exist_ok=True)\n",
    "intermidiate_DataFramesPath = os.path.abspath(os.path.join(\".\", \"Outputs\", \"intermidiate_DataFrames\"))\n",
    "os.makedirs(intermidiate_DataFramesPath, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time, load dataframes from the pre-run analysis\n",
    "for i in os.listdir(intermidiate_DataFramesPath):\n",
    "    name = i.split(\".\")[0]\n",
    "    tempframe = pd.read_csv(os.path.join(intermidiate_DataFramesPath, i), index_col=0)\n",
    "    globals()[str(name)] = tempframe   # This is sly, use globals to define an in-memory variable \n",
    "#osds_path = os.path.join(tempspace, \"OSDS_cleaned.shp\")\n",
    "\n",
    "# This takes the directory of CSV files from intermidiate_DataFramesPath, if the code has been run yet) to dynamically\n",
    "# load each CSV into a separate global variable, where the variable name is based on the CSV file's name.\n",
    "# In essence, it's automatically loading each CSV into memory as a DataFrame and naming the variables after the CSV file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download large files >100mb from offsite repo \n",
    "note this has a long run time! \n",
    "\n",
    "- Some of the files this notebook depends on are too large to be hosted by GitHub (The DEM is 1.2 GB)\n",
    "- One solution is to place these files into a cloud-based repositiry, in this case I use Digital Ocean spaces, and then just download them at the beginning of the work session\n",
    "- Once the analysis is complete You can either use the cell at the bottom of the notebook to remove these files from your Github Repo so any updates dont include the large files OR the folder specified here is in the GITIGNORE file and will not be pushed back up to the repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grab the Lidar elevation data: \n",
    "#LIDAR_file_urls = ['https://hcpt-large-file-storage.sfo3.digitaloceanspaces.com/Lidar_DEM_molokai/Lidar_DEM/MoKaiLiDar.tfw', \n",
    "#            'https://hcpt-large-file-storage.sfo3.digitaloceanspaces.com/Lidar_DEM_molokai/Lidar_DEM/MoKaiLiDar.tif.aux.xml',\n",
    "#            'https://hcpt-large-file-storage.sfo3.digitaloceanspaces.com/Lidar_DEM_molokai/Lidar_DEM/MoKaiLiDar.tif.xml', \n",
    "#            'https://hcpt-large-file-storage.sfo3.digitaloceanspaces.com/Lidar_DEM_molokai/Lidar_DEM/MoKaiLiDar.tif.ovr', \n",
    "#            'https://hcpt-large-file-storage.sfo3.digitaloceanspaces.com/Lidar_DEM_molokai/Lidar_DEM/MoKaiLiDar.tif']\n",
    "\n",
    "#LIDAR_local_folder = os.path.join(\"..\", \"Projected_data\\\\DEMs\\\\Lidar_DEM\")\n",
    "\n",
    "#download_files_from_public_endpoint(LIDAR_file_urls, LIDAR_local_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import OSDS File \n",
    "##### This creates a clean OSDS file from which all other cells will pull \n",
    "Currently using OSDS_v6 which incorporates 2020 updates from the DoH Wastewater branch (v3) IWS Database (V4) as well as the 4 counties tax records dwelling databases (V5).  This can easily be substituted as updates to the OSDS inventory are created.  This version wa run specifically for Molokai based on the Molokai V5 OSDS dataset made for Nancy at DHHL\n",
    "This adds 1280 cesspools to the tool, thereby increasing the number of cesspools in the tool to 82141 + 1280   = 83421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecuteError",
     "evalue": "Failed to execute. Parameters are not valid.\nERROR 000732: Input Features: Dataset C:\\Users\\CS\\Desktop\\Git_Repos\\HCPT_Molokai\\Projected_data\\OSDS_v7_Cleaned\\testbong23.shp does not exist or is not supported\nFailed to execute (CopyFeatures).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExecuteError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read in latest OSDS dataset  has to have an X and Y column in WGS 84\u001b[39;00m\n\u001b[0;32m      2\u001b[0m fromshp \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Projected_data/OSDS_v7_Cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtestbong23.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m OSDS \u001b[38;5;241m=\u001b[39m Format_OSDS_shp(fromshp, cleanitup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Deal with Dupes\u001b[39;00m\n\u001b[0;32m      7\u001b[0m checkdup \u001b[38;5;241m=\u001b[39m OSDS\u001b[38;5;241m.\u001b[39mpivot_table(index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUid\u001b[39m\u001b[38;5;124m'\u001b[39m], aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Git_Repos\\HCPT_Molokai\\Scripts\\functions_v2.py:30\u001b[0m, in \u001b[0;36mFormat_OSDS_shp\u001b[1;34m(fromshp, cleanitup)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFormat_OSDS_shp\u001b[39m(fromshp, cleanitup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m     osds_temp   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tempspace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOSDS_temp.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     arcpy\u001b[38;5;241m.\u001b[39mCopyFeatures_management(fromshp, osds_temp)   \u001b[38;5;66;03m# first make a copy\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Create UID\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     arcpy\u001b[38;5;241m.\u001b[39mmanagement\u001b[38;5;241m.\u001b[39mAddField(osds_temp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\management.py:6656\u001b[0m, in \u001b[0;36mCopyFeatures\u001b[1;34m(in_features, out_feature_class, config_keyword, spatial_grid_1, spatial_grid_2, spatial_grid_3)\u001b[0m\n\u001b[0;32m   6654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[0;32m   6655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 6656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\management.py:6647\u001b[0m, in \u001b[0;36mCopyFeatures\u001b[1;34m(in_features, out_feature_class, config_keyword, spatial_grid_1, spatial_grid_2, spatial_grid_3)\u001b[0m\n\u001b[0;32m   6643\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marcpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjectconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convertArcObjectToPythonObject\n\u001b[0;32m   6645\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6646\u001b[0m     retval \u001b[38;5;241m=\u001b[39m convertArcObjectToPythonObject(\n\u001b[1;32m-> 6647\u001b[0m         gp\u001b[38;5;241m.\u001b[39mCopyFeatures_management(\n\u001b[0;32m   6648\u001b[0m             \u001b[38;5;241m*\u001b[39mgp_fixargs(\n\u001b[0;32m   6649\u001b[0m                 (in_features, out_feature_class, config_keyword, spatial_grid_1, spatial_grid_2, spatial_grid_3),\n\u001b[0;32m   6650\u001b[0m                 \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   6651\u001b[0m             )\n\u001b[0;32m   6652\u001b[0m         )\n\u001b[0;32m   6653\u001b[0m     )\n\u001b[0;32m   6654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[0;32m   6655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py:533\u001b[0m, in \u001b[0;36mGeoprocessor.__getattr__.<locals>.<lambda>\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    531\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gp, attr)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(val):\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: val(\u001b[38;5;241m*\u001b[39mgp_fixargs(args, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convertArcObjectToPythonObject(val)\n",
      "\u001b[1;31mExecuteError\u001b[0m: Failed to execute. Parameters are not valid.\nERROR 000732: Input Features: Dataset C:\\Users\\CS\\Desktop\\Git_Repos\\HCPT_Molokai\\Projected_data\\OSDS_v7_Cleaned\\testbong23.shp does not exist or is not supported\nFailed to execute (CopyFeatures).\n"
     ]
    }
   ],
   "source": [
    "# Read in latest OSDS dataset  has to have an X and Y column in WGS 84\n",
    "fromshp = os.path.join(os.path.abspath(\"../Projected_data/OSDS_v7_Cleaned\"), \"CESSPOOLS_v7_Cleaned.shp\")\n",
    "\n",
    "OSDS = Format_OSDS_shp(fromshp, cleanitup=True)\n",
    "\n",
    "# Deal with Dupes\n",
    "checkdup = OSDS.pivot_table(index=['Uid'], aggfunc='size')\n",
    "#print(f\"there are duplicate points at: {checkdup[checkdup != 1]}\")\n",
    "# remove the duplicates (keep first entry of the duplicate row)\n",
    "OSDS.drop_duplicates(subset=['Uid'], keep=\"first\", inplace=True)\n",
    "# Reindex or else the future joins will be based on an index with dupes\n",
    "OSDS.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save csv\n",
    "OSDS.to_csv(os.path.join(intermidiate_DataFramesPath, \"OSDS.csv\"))\n",
    "\n",
    "# Convert the now formatted OSDS csv back to a shapefile for the subsequent analyses\n",
    "csvFilePath = os.path.join(intermidiate_DataFramesPath, \"OSDS.csv\")\n",
    "convert_OSDS_csv_to_shp(csvFilePath)\n",
    "\n",
    "# Define the cleaned up OSDS file all the other cells can use\n",
    "osds_path = os.path.join(tempspace, \"OSDS_cleaned.shp\")\n",
    "print(\"There are {} (OSDS or Cesspools) in this dataset\".format(len(OSDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance to Municipal or domestic wells \n",
    "\n",
    "Refs: Whittier 2010 Groundwater source assessment program for the state of Hawaii, USA: methodology and example application Robert B. Whittier & Kolja Rotzoll & Sushant Dhal & Aly I. El-Kadi & Chittaranjan Ray & Daniel Chang\n",
    "\n",
    "Mair, A., & El-Kadi, A. I. (2013). Logistic regression modeling to assess groundwater vulnerability to contamination in Hawaii, USA. Journal of contaminant hydrology, 153, 1-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate parameter info about the Municipal wells MODERATE RUN TIME\n",
    "shp_path = os.path.join(\"..\", \"Projected_data/Wells\", 'CWRM_Wells_MUN_prj.shp')\n",
    "new_col_name = \"dist2_MunWells_m\"\n",
    "# Run the near analysis \n",
    "MUN_WELLS = Calc_dist_to_variable(osds_path, shp_path, new_col_name)\n",
    "MUN_WELLS.to_csv(os.path.join(intermidiate_DataFramesPath, \"MUN_WELLS.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(MUN_WELLS, \"dist2_MunWells_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate paramater info about the Domestic wells MODERATE RUN TIME\n",
    "shp_path = os.path.join(\"..\", \"Projected_data/Wells\", 'CWRM_Wells_DOM_prj.shp')\n",
    "new_col_name = \"dist2_DomWells_m\"\n",
    "# Run the near analysis \n",
    "DOM_WELLS = Calc_dist_to_variable(osds_path, shp_path, new_col_name)\n",
    "DOM_WELLS.to_csv(os.path.join(intermidiate_DataFramesPath, \"DOM_WELLS.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(DOM_WELLS, \"dist2_DomWells_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well capture zones\n",
    "\n",
    "While the distance from the well as related to the zone threshold was directly assessed we also did an analysis of which osds were located in municipal well capture zones provided by the HI department of health. The composite capture zone dataset as it is publically available and presented in the references below was provided upon request by Bob Whittier at Hi-DOH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inside Well capture Zone B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_polygons = os.path.join(\"..\", \"Projected_data/Well_CZs\", 'ZoneB_Composite_Molokai.shp')\n",
    "new_col_name = 'In_WellCZ_B'\n",
    "In_points = osds_path\n",
    "\n",
    "WellCZ_B_yes = Find_points_inside_polys(In_points, In_polygons, new_col_name)\n",
    "WellCZ_B_yes.to_csv(os.path.join(intermidiate_DataFramesPath, \"WellCZ_B_yes.csv\"))\n",
    "\n",
    "print_stats_on_bool_layers(WellCZ_B_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inside Well capture Zone C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_polygons = os.path.join(\"..\", \"Projected_data/Well_CZs\", 'ZoneC_Composite_Molokai.shp')\n",
    "new_col_name = 'In_WellCZ_C'\n",
    "In_points = osds_path\n",
    "\n",
    "WellCZ_C_yes = Find_points_inside_polys(In_points, In_polygons, new_col_name)\n",
    "WellCZ_C_yes.to_csv(os.path.join(intermidiate_DataFramesPath, \"WellCZ_C_yes.csv\"))\n",
    "\n",
    "print_stats_on_bool_layers(WellCZ_C_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance to streams\n",
    "\n",
    "Streams layer from Hawaii GIS online is representative of perennial and perennial and non-perennial flowing waters in the state as of 2008. This layer is sometimes referred to as \"DAR Streams.\" Source: USGS Digital Line Graphs, 1983 version; CWRM Hawaii Stream Assessment database, 1993; DLNR Division of Aquatic Resources, 2004, 2008, 2013. (Note - 2013 update from CWRM and DAR included attribute corrections and addition of tributary names, data is current as of March, 2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate parameter info about the Municipal wells MODERATE RUN TIME\n",
    "shp_path = os.path.join(\"..\", \"Projected_data/Streams\", 'Streams_prj.shp')\n",
    "new_col_name = \"dist2_Streams_m\"\n",
    "# Run the near analysis \n",
    "STREAMS = Calc_dist_to_variable(osds_path, shp_path, new_col_name)\n",
    "STREAMS.to_csv(os.path.join(intermidiate_DataFramesPath, \"STREAMS.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(STREAMS, \"dist2_Streams_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance to Wetlands\n",
    "\n",
    "Using a basic geographic distance calculation to the nearest point on the streams shapefile from: HAWAII GIS:(https://geoportal.hawaii.gov/datasets/2250370303ef44c8becf4fbe549e3f43_39, https://www.arcgis.com/sharing/rest/content/items/2250370303ef44c8becf4fbe549e3f43/info/metadata/metadata.xml?format=default&output=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate parameter info about the wetlands\n",
    "shp_path = os.path.join(\"..\", \"Projected_data/Wetlands\", 'Wetlands_emergent_ponds.shp')\n",
    "new_col_name = \"dist2_Wetlands_m\"\n",
    "# Run the near analysis \n",
    "WETLANDS = Calc_dist_to_variable(osds_path, shp_path, new_col_name)\n",
    "WETLANDS.to_csv(os.path.join(intermidiate_DataFramesPath, \"WETLANDS.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(WETLANDS, \"dist2_Wetlands_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance to the coastline\n",
    "\n",
    "Here we are using a basic geographic distance calculation to the nearest point on the Hawaii coastline shapefile taken from Hawaii GIS online. (https://geoportal.hawaii.gov/datasets/045b1d5147634e2380566668e04094c6_3/explore?location=20.556650%2C-157.478000%2C8.60, https://geodata.hawaii.gov/arcgis/rest/services/Terrestrial/MapServer/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate parameter info about the COASTLINE   looong RUN TIME \n",
    "shp_path = os.path.join(\"..\", \"Projected_data/Coastline\", 'Coastline_line_prj.shp')\n",
    "new_col_name = \"dist2_coast_m\"\n",
    "# Run the near analysis \n",
    "COASTLINE = Calc_dist_to_variable(osds_path, shp_path, new_col_name)\n",
    "COASTLINE.to_csv(os.path.join(intermidiate_DataFramesPath, \"COASTLINE.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(COASTLINE, \"dist2_coast_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting values from Rainfall raster \n",
    "(note need to run this cell before running soils block below) \n",
    "\n",
    "- Rainfall directly affects the contamination potential of an OSDS by way of being a proxy for groundwater recharge, whereas:\n",
    "- a moderate amount of OSDS leachate can significantly increase the groundwater nitrate concentration in low groundwater recharge areas such as the west Hawaii Island.  By contrast, it will take a much greater amount of OSDS leachate to result in even a moderate increase in the groundwater nitrate concentration in high recharge area such as the east side of Hawaii island. \n",
    "\n",
    "\n",
    "##### Using rainfall data to fill missing soil flooding values\n",
    "Addiitionally, rainfall can also act as a proxy for flooding potential, which can cause surface transport of contamination from OSDS, especially cesspools, and cause public health risks. The flood frequency of many locations is already defined by the NRCS soils database, though about fifty (50) percent of the cesspool points were missing flood-frequency data from the NRCS database. Therefore, rainfall amounts were used to determine the likelihood of flooding only in those areas where it was not defined by the NRCS database using the following logic. This logic was established through consultation with DOHWWB and engineering professionals \n",
    "\n",
    "- If rainfall is above 135 in and ksat is < 1.1 = Frequent Flooding\n",
    "- If rainfall is below 15 in and ksat is < 1.1 = Frequent Flooding\n",
    "- If rainfall is above 135 or ksat is below ksat < 1.1 = Occasional Flooding\n",
    "- The rainfall data itself was pulled from the Hawai‘i climate atlas as statewide grids of annual rainfall totals. \n",
    "(http://rainfall.geography.hawaii.edu/)\n",
    "\n",
    "To associate the rainfall amounts we extracted the annual rainfall value (in inches) from the rainfall raster to each OSDS point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_raster = os.path.join(\"..\", \"Projected_data/Rain\", 'Rain_inann.tif')\n",
    "In_points = osds_path\n",
    "new_col_name = \"rainfall_in\"\n",
    "\n",
    "RAINFALL = extract_values_from_rasters(In_raster, In_points, new_col_name, tempspace)\n",
    "RAINFALL.to_csv(os.path.join(intermidiate_DataFramesPath, \"RAINFALL.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(RAINFALL, \"rainfall_in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soils \n",
    "\n",
    "- These data were extracted data straight from the NRCS database(https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/survey/geo/?cid=nrcs142p2_053627) , (Ksat data is much better resolution than with the Hawaii soil atlas, which we also looked at https://gis.ctahr.hawaii.edu/SoilAtlas). \n",
    "\n",
    "\n",
    "- As noted above: FloodFrequency variable was missing in about half of features so it is filled in with combination of rainfall and k-sat below. \n",
    "\n",
    "\n",
    "- Because soil data is polygon data we simply assigned each OSDS point the value of the soil parameter of interest. \n",
    "- Parameters included: \n",
    "  - **Flood Frequency** - Degree to which the soil is subject to flooding or ponding (nrcs_FloodFreq)\n",
    "  - **Depth to bedrock** or cemented pan (nrcs_DtoBrock)\n",
    "  - **Filtering characteristics of the soil** (nrcs_ksat_rep) with ksat >12 ft/day being poor filtering capacity\n",
    "  - **Rate of water infiltration** through the soil (nrcs_ksat_rep) with ksat < 1.2 ft/day being poor infiltration capacity \n",
    "  - **Rate of seepage** out of the bottom layer of the soil (nrcs_ksat_rep) with ksat < 1.1 ft/day being poor seepage\n",
    "  - **Topographic slope** with slope of more than 15% being problematic for OSDS installation and conversions \n",
    "  - **Fraction of rock fragments** in the soil with a % of 3-in rock fragments more than 50% being problematic. \n",
    "  - Note that meaning can derived from thresholds in the table in Whittier and El-Kadi (2014) on pg 7-4 (see that table exact limitation numbers) https://health.hawaii.gov/wastewater/files/2015/09/OSDS_NI.pdf\n",
    "  \n",
    "  \n",
    "- These parameters followed the methodology and weighting scheme of Whittier and El-Kadi (2014) (pg 7-4) to create a single soil suitability ranking\n",
    "\n",
    "\n",
    "- note that the NRCS septic suitability rank was of little value (it was all bad) which is also noted in Whittier and El-Kadi (2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in soils data from NRCS\n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/Soils\", 'HIstate_nrcs_join2.shp')\n",
    "new_col_name = 'Soil_Ksat_2ndry'\n",
    "In_points = osds_path\n",
    "\n",
    "# Columns you want from the shapefile (Uid should always  be first one) \n",
    "want_cols = ['Uid', 'brockdepmin', 'flodfreqdcd', 'engstafdcd', 'engstafll', \n",
    "             'engstafml', 'cokey', 'ksat_h', 'ksat_l', 'ksat_r', 'sieveno10_r', ]  # 'totalsub_r', slopegradw\n",
    "\n",
    "renames = {'brockdepmi':'nrcs_DtoBrock', \n",
    " 'flodfreqdc':'nrcs_FloodFreq',\n",
    " 'engstafdcd':'nrcs_septic_dominant', \n",
    " 'engstafll':'nrcs_septic_least', \n",
    " 'engstafml':'nrcs_septic_most', \n",
    " 'ksat_h':'nrcs_ksat_hi',  \n",
    " 'ksat_l':'nrcs_ksat_lo',\n",
    " 'ksat_r':'nrcs_ksat_rep', \n",
    " 'sieveno10_':'nrcs_rockFrag3in'}    # 'slopegradw':'nrcs_slope', 'totalsub_r':'nrcs_subsidence'\n",
    "\n",
    "# Do the spatial join\n",
    "arcpy.SpatialJoin_analysis(In_points, In_polygons, os.path.join(tempspace, \"test_join_pt_2_poly.shp\"))\n",
    "\n",
    "# read data from shapefile \n",
    "extracted_points_path = os.path.join(tempspace, \"test_join_pt_2_poly.shp\")\n",
    "arcpy.TableToTable_conversion(extracted_points_path, tempspace, 'extracted_all.csv') # Create a rational file format\n",
    "# Create a rational PanDataframe\n",
    "Extracted_All = pd.read_csv(os.path.join(tempspace, 'extracted_all.csv'))\n",
    "\n",
    "want_cols = ['Uid', 'brockdepmi', 'flodfreqdc', 'engstafdcd', 'engstafll', \n",
    "             'engstafml', 'ksat_h', 'ksat_l', 'ksat_r', 'sieveno10_'] # 'slopegradw' , 'totalsub_r'\n",
    "Extract_frame = Extracted_All[want_cols]\n",
    "\n",
    "Extract_frame = Extract_frame.replace('NoData', np.nan)  # clean up from arc's trashiness\n",
    "\n",
    "numcols = ['brockdepmi', 'ksat_h', 'ksat_l', 'ksat_r', 'sieveno10_' ]  #'slopegradw',  'totalsub_r'\n",
    "Extract_frame[numcols] = Extract_frame[numcols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "Extract_frame.rename(columns=renames, inplace=True)     # Rename to col that you want\n",
    "\n",
    "SOILS_NRCS = Extract_frame.copy()\n",
    "\n",
    "\n",
    "#### Filling in missing NRCS flood frequency data with rainfall and ksat\n",
    "SOILS_NRCS = SOILS_NRCS.merge(RAINFALL, on=\"Uid\", how='left')\n",
    "\n",
    "# Set frequents \n",
    "SOILS_NRCS.loc[(SOILS_NRCS['nrcs_FloodFreq'].isnull()) & (SOILS_NRCS['nrcs_ksat_rep'] < 1.1) & (SOILS_NRCS['rainfall_in'] > 135), 'nrcs_FloodFreq'] = \"Frequent\"\n",
    "SOILS_NRCS.loc[(SOILS_NRCS['nrcs_FloodFreq'].isnull()) & (SOILS_NRCS['nrcs_ksat_rep'] < 1.1) & (SOILS_NRCS['rainfall_in'] < 15), 'nrcs_FloodFreq'] = \"Frequent\"\n",
    "\n",
    "\n",
    "# Set Occasionals \n",
    "SOILS_NRCS.loc[(SOILS_NRCS['nrcs_FloodFreq'].isnull()) & (SOILS_NRCS['rainfall_in'] > 135), 'nrcs_FloodFreq'] = \"Occasional\"\n",
    "SOILS_NRCS.loc[(SOILS_NRCS['nrcs_FloodFreq'].isnull()) & (SOILS_NRCS['nrcs_ksat_rep'] < 1.1), 'nrcs_FloodFreq'] = \"Occasional\"\n",
    "\n",
    "# set rest to none\n",
    "SOILS_NRCS.loc[SOILS_NRCS['nrcs_FloodFreq'].isnull(), 'nrcs_FloodFreq'] = \"None\"\n",
    "\n",
    "del SOILS_NRCS['rainfall_in']\n",
    "\n",
    "SOILS_NRCS.to_csv(os.path.join(intermidiate_DataFramesPath, \"SOILS_NRCS.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope\n",
    "Data is derived from statewide 10 m DEMs from the Geology coastal group page (http://www.soest.hawaii.edu/coasts/data/hawaii/dem.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blankframe = pd.DataFrame(columns=[\"Uid\"])\n",
    "\n",
    "indir = os.path.join(\"..\", \"Projected_data\\\\DEMs\\\\Slope_WGS\")  # Using a loop to go over different rasters from different islands\n",
    "for idx, file in enumerate(os.listdir(indir)):\n",
    "    if file.endswith(\".tif\"):\n",
    "        In_raster = os.path.join(indir, file)\n",
    "        In_points = osds_path\n",
    "        new_col_name = \"slope_{}\".format(idx)       \n",
    "        foo = extract_values_from_rasters(In_raster, In_points, new_col_name, tempspace)\n",
    "        blankframe = blankframe.merge(foo, how='outer', on='Uid')\n",
    "\n",
    "blankframe.set_index(\"Uid\", inplace=True)\n",
    "blankframe['slope_deg'] = blankframe.max(axis=1)  \n",
    "blankframe.reset_index(inplace=True)\n",
    "\n",
    "SLOPE = blankframe[[\"Uid\", 'slope_deg']]\n",
    "SLOPE.to_csv(os.path.join(intermidiate_DataFramesPath, \"SLOPE.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(SLOPE, \"slope_deg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth to water \n",
    "#### Water table Elevations\n",
    "Did a bit of processing (in tif format in WGS 84) from Bob Whittier's models (obtained through personal communication) \n",
    "\n",
    "- The unsaturated zone underlying a cesspool or leachfield is the primary site of subsurface treatment and contaminant attenuation, and the thickness of the unsaturated zone below an OSDS is a major factor in the ability of a given OSDS to treat wastewater.  \n",
    "- To obtain an estimate of depth to water I first needed to extract ESTIMATED/modeled WT elevations from Bob's groundwater models.  The best citations for these models are:  \n",
    " \n",
    " \n",
    "- Whittier, R.B., and El-Kadi, A. I. (2009). Human and environmental risk ranking of onsite sewage disposal systems. Retrieved from: https://health.hawaii.gov/wastewater/files/2015/09/OSDS_OAHU.pdf\n",
    "- Whittier, R.B., and El-Kadi, A. I. (2014). Human health and environmental risk ranking of onsite sewage disposal systems for the Hawaiian Islands of Kaua'i, Moloka'i, Maui, and Hawai'i. Retrieved from https://health.hawaii.gov/wastewater/files/2015/09/OSDS_NI.pdf\n",
    " \n",
    " \n",
    "- Water table depth was calculated in combination with land surface elevation. \n",
    "- Unfortunately There is no statewide high-resolution (<1 m dem) lidar data for the state, only for the coastlines. Therefore I downloaded the coastline lidar data from HI GIS online (https://www.arcgis.com/home/webmap/viewer.html?url=https%3A%2F%2Fgeodata.hawaii.gov%2Farcgis%2Frest%2Fservices%2FSoH_Imagery%2FHawaii_DSM%2FImageServer&source=sd) and extracted elevation data to each OSDS point that intersected the available data. \n",
    "- Then I filled in missing elevation values with a lower resolution data set (10 m DEMS)  from the UH Coastal Geology group (http://www.soest.hawaii.edu/coasts/data/hawaii/dem.html) \n",
    " \n",
    "- With the best possible land surface elevation data, and the modeled water table elevations, we subtracted the land elevation from the water table elevation to derive the estimated depth to water. \n",
    "- While this is a modeled estimate it is the best available data for a notoriously uncertain parameter.\n",
    "\n",
    "##### Justification for threshold used:  \n",
    "From Whittier 2014 pg 4-2 \n",
    "In this study, the minimum depth to groundwater that allows for sufficient pathogen removal was based on the OSDS design regulations for cesspools. HAR Title 11, Chapter 62 establishes the regulations for wastewater systems and requires that:\n",
    "• the top of a cesspool inlet pipe must be 1.5 ft below grade;\n",
    "• there is a minimum of 10 ft between the inlet pipe and the bottom of the tank; and\n",
    "• a minimum of 3 ft from the bottom of the tank and the highest known level of groundwater.\n",
    "Based on these values, a minimum depth to the water table should be approximately 15 ft. However, the analysis can be uncertain due to the fact that the elevation of the water table is not static;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blankframe = pd.DataFrame(columns=[\"Uid\"])\n",
    "indir = os.path.join(\"..\", \"Projected_data\\\\Water_table\\\\Tiffs\")\n",
    "\n",
    "for idx, file in enumerate(os.listdir(indir)):     # Using a loop to go over different rasters from different islands\n",
    "    if file.endswith(\".tif\"):\n",
    "        In_raster = os.path.join(indir, file)\n",
    "        In_points = osds_path\n",
    "        new_col_name = \"WT_elev_m_{}\".format(idx)\n",
    "         \n",
    "        foo = extract_values_from_rasters(In_raster, In_points, new_col_name, tempspace)   \n",
    "        # Merge the different islands together \n",
    "        blankframe = blankframe.merge(foo, how='outer', on='Uid')\n",
    "\n",
    "blankframe.set_index(\"Uid\", inplace=True)\n",
    "blankframe['WT_elev_m'] = blankframe.max(axis=1)  \n",
    "blankframe.reset_index(inplace=True)\n",
    "\n",
    "WT_ELEV = blankframe[[\"Uid\", 'WT_elev_m']]\n",
    "WT_ELEV.to_csv(os.path.join(intermidiate_DataFramesPath, \"WT_ELEV.csv\"))\n",
    "# plot histogram\n",
    "Hist_and_stats_on_DF(WT_ELEV, \"WT_elev_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Surface elevations\n",
    "This relies on first getting data from lidar dem (1m resolution), then if no data get data from the course (10m) resolution ones to create cohesive dataset\n",
    "\n",
    "- Worst data downloading experience ever was the lame 10m dem  \n",
    "- https://pae-paha.pacioos.hawaii.edu/erddap/griddap/usgs_dem_10m_molokai.html\n",
    "\n",
    "##### Very long run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blankframe = pd.DataFrame(columns=[\"Uid\"])\n",
    "indir_crap = os.path.join(\"..\", \"Projected_data\\\\DEMs\\\\10mDEM_WGS\")\n",
    "\n",
    "for idx, file in enumerate(os.listdir(indir_crap)):   # Using a loop to go over different rasters from different islands\n",
    "    if file.endswith(\".tif\"):\n",
    "        print(\"working on {}\".format(file))\n",
    "        In_raster = os.path.join(indir_crap, file)\n",
    "        In_points = osds_path\n",
    "        # Define new collumn name you want\n",
    "        new_col_name = \"CrapElev_m_{}\".format(idx)\n",
    "        \n",
    "        foo = extract_values_from_rasters(In_raster, In_points, new_col_name, tempspace)   \n",
    "        # Merge the different islands together \n",
    "        blankframe = blankframe.merge(foo, how='outer', on='Uid')\n",
    "        \n",
    "blankframe.set_index(\"Uid\", inplace=True)\n",
    "blankframe['CrapElev'] = blankframe.max(axis=1)  \n",
    "blankframe.reset_index(inplace=True)\n",
    "\n",
    "ELEV_crap = blankframe[[\"Uid\", 'CrapElev']]\n",
    "\n",
    "\n",
    "####  Now do the same for the lidar dataset  to stitch on ####\n",
    "blankframe = pd.DataFrame(columns=[\"Uid\"])\n",
    "indir_good = os.path.join(\"..\", \"Projected_data\\\\DEMs\\\\Lidar_DEM\")\n",
    "\n",
    "for idx, file in enumerate(os.listdir(indir_good)):   # Using a loop to go over different rasters from different islands\n",
    "    if file.endswith(\".tif\"):\n",
    "        print(\"working on {}\".format(file))\n",
    "        In_raster = os.path.join(indir_good, file)\n",
    "        In_points = osds_path\n",
    "        # Define new collumn name you want\n",
    "        new_col_name = \"LidarElev_m\"\n",
    "        \n",
    "        foo = extract_values_from_rasters(In_raster, In_points, new_col_name, tempspace)   \n",
    "        # Merge the different islands together \n",
    "        blankframe = blankframe.merge(foo, how='outer', on='Uid')\n",
    "\n",
    "blankframe.set_index(\"Uid\", inplace=True)\n",
    "blankframe['LidarElev'] = blankframe.max(axis=1)  \n",
    "blankframe.reset_index(inplace=True)\n",
    "\n",
    "ELEV_good = blankframe[[\"Uid\", 'LidarElev']]\n",
    "\n",
    "# Now Merge Elevations to create a master elevation\n",
    "Master = OSDS.copy()\n",
    "\n",
    "Master = Master.merge(ELEV_good, on=\"Uid\", how=\"left\")\n",
    "Master = Master.merge(ELEV_crap, on=\"Uid\", how=\"left\")\n",
    "\n",
    "Master['Elevation_m'] = Master['LidarElev']\n",
    "\n",
    "Master['Elevation_m'] = np.where((Master['LidarElev'].isnull()),   #Identifies the case to apply to \n",
    "                           Master['CrapElev'],      #This is the value that is inserted\n",
    "                           Master['Elevation_m'])      #This is the column that is affected\n",
    "\n",
    "ELEVATION = Master[[\"Uid\", 'Elevation_m']]\n",
    "ELEVATION.to_csv(os.path.join(intermidiate_DataFramesPath, \"ELEVATION.csv\"))\n",
    "\n",
    "Hist_and_stats_on_DF(ELEVATION, \"Elevation_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Depth to Water for all OSDS points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DtoWater = WT_ELEV.merge(ELEVATION, on=\"Uid\", how='outer')                     # merge elevation and WT elevation\n",
    "DtoWater['Dep_to_Water_m'] = DtoWater['Elevation_m'] - DtoWater['WT_elev_m']   # find the difference \n",
    "\n",
    "DtoWater['Dep_to_Water_m'][DtoWater['Dep_to_Water_m'] < 0] = 0.999        # Where it is negative assign value of .999\n",
    "\n",
    "DTOWATER = DtoWater.copy()\n",
    "DTOWATER.to_csv(os.path.join(intermidiate_DataFramesPath, \"DTOWATER.csv\"))\n",
    "\n",
    "Hist_and_stats_on_DF(DTOWATER, \"Dep_to_Water_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination, horizontal + Vertical SLR effects\n",
    "- This is a combination metric, composed of systems that EITHER are inside of the lateral 2d SLR zones as described by the scenarios below OR have a depth to groundwater value that becomes less than 4.4 m (the regulatory standard via 11-62) with a given increase in sea level (assuming purely linear hydrodynamic buoyancy of the freshwater lens under an increase in Base Sea Level) that matches the increase in Sea Level elevation for the future-scenario year.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if inside SLR zone 2030\n",
    "\n",
    "Sea level rise data was collected from the HI GIS online https://geoportal.hawaii.gov/datasets/5a4e4fdac72e40bd846c8f3257afd4aa_59?geometry=-163.283%2C20.530%2C-152.829%2C22.320\n",
    "\n",
    "- OSDS units are assigned highest priority rank if located within 2030 zone with descending ranks if located in 2050 to 2100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile systems affected by Vertical SLR inundation\n",
    "Vertical_SLR_2030 = DTOWATER[DTOWATER['Dep_to_Water_m'] < 4.9]\n",
    "print(\"vertical stats\"); print_stats_on_bool_layers(Vertical_SLR_2030)\n",
    "\n",
    "# Compile systems in the horizontal surface flood zone\n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/SLR\", 'slr_exposure_area_0_pt_5_ft.shp')\n",
    "new_col_name = 'InSLR2030'\n",
    "In_points = osds_path\n",
    "Hor_SLR_2030 = Find_points_inside_polys(In_points, In_polygons, new_col_name)\n",
    "print(\"Hor stats\"); print_stats_on_bool_layers(Hor_SLR_2030)\n",
    "\n",
    "SLR_2030_yes = Vertical_SLR_2030.merge(Hor_SLR_2030, on=\"Uid\", how=\"outer\")\n",
    "SLR_2030_yes = SLR_2030_yes[['Uid', new_col_name]]\n",
    "SLR_2030_yes[new_col_name] = True   # be sure to set all the rows that are in hor or vert zone to true\n",
    "\n",
    "SLR_2030_yes.to_csv(os.path.join(intermidiate_DataFramesPath, \"SLR_2030_yes.csv\"))\n",
    "print(\"Vert+Hor stats\"); print_stats_on_bool_layers(SLR_2030_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if inside SLR zone 2050 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile systems affected by Vertical SLR inundation\n",
    "Vertical_SLR_2050 = DTOWATER[DTOWATER['Dep_to_Water_m'] < 5.5]\n",
    "print(\"vertical stats\"); print_stats_on_bool_layers(Vertical_SLR_2050)\n",
    "\n",
    "# Compile systems in the horizontal surface flood zone\n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/SLR\", 'slr_exposure_area_1_pt_1_ft.shp')\n",
    "new_col_name = 'InSLR2050'\n",
    "In_points = osds_path\n",
    "Hor_SLR_2050 = Find_points_inside_polys(In_points, In_polygons, new_col_name)\n",
    "print(\"Hor stats\"); print_stats_on_bool_layers(Hor_SLR_2050)\n",
    "\n",
    "SLR_2050_yes = Vertical_SLR_2050.merge(Hor_SLR_2050, on=\"Uid\", how=\"outer\")\n",
    "SLR_2050_yes = SLR_2050_yes[['Uid', new_col_name]]\n",
    "SLR_2050_yes[new_col_name] = True  # be sure to set all the rows that are in hor or vert zone to true\n",
    "\n",
    "SLR_2050_yes.to_csv(os.path.join(intermidiate_DataFramesPath, \"SLR_2050_yes.csv\"))\n",
    "print(\"Vert+Hor stats\"); print_stats_on_bool_layers(SLR_2050_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if inside SLR zone 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile systems affected by Vertical SLR inundation\n",
    "Vertical_SLR_2100 = DTOWATER[DTOWATER['Dep_to_Water_m'] < 7.6]\n",
    "print(\"vertical stats\"); print_stats_on_bool_layers(Vertical_SLR_2100)\n",
    "\n",
    "\n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/SLR\", 'slr_exposure_area_3_pt_2_ft.shp')\n",
    "new_col_name = 'InSLR2100'\n",
    "In_points = osds_path\n",
    "Hor_SLR_2100 = Find_points_inside_polys(In_points, In_polygons, new_col_name)\n",
    "print(\"Hor stats\"); print_stats_on_bool_layers(Hor_SLR_2100)\n",
    "\n",
    "SLR_2100_yes = Vertical_SLR_2100.merge(Hor_SLR_2100, on=\"Uid\", how=\"outer\")\n",
    "SLR_2100_yes = SLR_2100_yes[['Uid', new_col_name]]\n",
    "SLR_2100_yes[new_col_name] = True    # be sure to set all the rows that are in hor or vert zone to true\n",
    "\n",
    "SLR_2100_yes.to_csv(os.path.join(intermidiate_DataFramesPath, \"SLR_2100_yes.csv\"))\n",
    "print(\"Vert+Hor stats\"); print_stats_on_bool_layers(SLR_2100_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate OSDS Density \n",
    "\n",
    "Careful parameterization is needed for this, as it can greatly affect the results. \n",
    "key questions are how far away from another OSDS does an OSDS need to be for their effects across the landscape to not be cumulative?  This is represented by the parameter:\n",
    "\n",
    "-  effect_distance\n",
    "\n",
    "We set this at 360m because personal communication with Joquim at WAI saying that most properties in Hawaii are 1 acre (which is incorrect). Therefore the base resolution of this density analysis is number of OSDS per acre, looking at a 1 acre cell size with a 10 acre search distance around each OSDS\n",
    "\n",
    "A cell size of 64 meters creates 1 acre cells\n",
    "\n",
    "- Then the threshold for prioritization was set at one units per 10000 sqft, apparently a thresholds in 11-62?\n",
    "- This translates to a per acre density of 4.23 units per acre. Note that this was adjusted to 1/4 of this value 1.089 because we are preferring to use a threshold of number of units per acre as it makes more sense in this geography, regarding the note above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the OSDS dots into UTM4n to get units of meters\n",
    "input_features = os.path.join(os.path.abspath(\"../Projected_data/OSDS_v7_Cleaned\"), \"CESSPOOLS_v7_Cleaned.shp\")\n",
    "OSDS_utm_path = os.path.join(tempspace, \"ClassIV_cespool_UTM4N.shp\" )  # replacing \"..\", \"Projected_data/OSDS/UTM\", \"ClassIV_cespool_UTM4N.shp\"\n",
    "out_coordinate_system = arcpy.SpatialReference('WGS 1984 UTM Zone  4N')\n",
    "arcpy.Project_management(input_features, OSDS_utm_path, out_coordinate_system)\n",
    "\n",
    "# Density Parameters\n",
    "effect_distance_m = 360.5\n",
    "Density_units = \"ACRES\"\n",
    "cellSize = 64     \n",
    "new_col_name = \"OSDS_Density_perAcre\"   # Define new collumn name you want\n",
    "\n",
    "# internal parameters\n",
    "myNbrCirc = NbrCircle(effect_distance_m, \"MAP\")\n",
    "populationField = \"NONE\"\n",
    "# Run density algo\n",
    "outPdens = PointDensity(OSDS_utm_path, populationField, cellSize, myNbrCirc, Density_units)\n",
    "outPdens.save(os.path.join(tempspace, 'CessPool_density.tif'))\n",
    "\n",
    "# Convert format to dataframe\n",
    "In_raster = outPdens\n",
    "In_points = osds_path\n",
    "ExtractValuesToPoints(In_points, In_raster, os.path.join(tempspace, \"test_rain_OSDS.shp\"))\n",
    "\n",
    "# read on OSDS data from shapefile into dataframe \n",
    "extracted_points_path = os.path.join(tempspace, \"test_rain_OSDS.shp\")\n",
    "arcpy.TableToTable_conversion(extracted_points_path, tempspace, 'extracted_all.csv') # Create a rational file format\n",
    "# Create a rational PanDataframe\n",
    "Extracted_All = pd.read_csv(os.path.join(tempspace, 'extracted_all.csv'))\n",
    "\n",
    "\n",
    "#Select only selected columns\n",
    "want_cols = ['Uid', 'RASTERVALU']\n",
    "Extract_frame = Extracted_All[want_cols]\n",
    "Extract_frame.rename(columns={'RASTERVALU':new_col_name}, inplace=True)     # Rename to col that you want\n",
    "Extract_frame.loc[Extract_frame[new_col_name] < 0] = np.nan                 # Deal with the -9999 values set to nan\n",
    "\n",
    "OSDS_DENSITY = Extract_frame.copy()\n",
    "OSDS_DENSITY.to_csv(os.path.join(intermidiate_DataFramesPath, \"OSDS_DENSITY.csv\"))\n",
    "\n",
    "Hist_and_stats_on_DF(OSDS_DENSITY, \"OSDS_Density_perAcre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Coastal Endpoint ID's from the Bob Model flowpaths\n",
    "\n",
    "Here referred to as 'Flikr_ID' these are just the 250 m points/cells all along the coastline for all islands with a unique ID that can be used to assign an inland unit to an area of impact along the coast\n",
    "\n",
    "###### Note to self, the whole Statewide_OSDS_Flikr_Paths_v6.shp needs to be regenerated if using an updated OSDS file. \n",
    "otherwise new points will end up with NaN or 0s for Flik ID and will not get assigned coral ranks or userdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Fliker_ID from the grid cells for each point\n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/ImpactZones_Flik_ID\", 'Molokai_grid_w_Flikr_IDs_MauiID.shp')\n",
    "In_points = osds_path\n",
    "\n",
    "# Do the spatial join\n",
    "arcpy.SpatialJoin_analysis(In_points, In_polygons, os.path.join(tempspace, \"test_join_pt_2_poly.shp\"))\n",
    "\n",
    "# Sub in the geographically closest Flikr cell if the OSDS didnt hit a GMS grid cell. and format the Final dataframe\n",
    "in_features = os.path.join(tempspace, \"test_join_pt_2_poly.shp\")\n",
    "Flikr_cells = os.path.join(\"..\", \"Projected_data/ImpactZones_Flik_ID/GeographicFlikr_data\", \"Flikr_MoKai_250_cells_wMidpoints.shp\")\n",
    "OSDS_FLIK_ID = deal_with_no_FlikrCellsOSDS_pts(in_features, Flikr_cells)\n",
    "\n",
    "OSDS_FLIK_ID.to_csv(os.path.join(intermidiate_DataFramesPath, \"OSDS_FLIK_ID.csv\"))\n",
    "\n",
    "print_costal_endpoint_flik_ID_analytics(OSDS_FLIK_ID, OSDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value based risk factors\n",
    "The risk factors presented above are generally based on physical factors that correspond to increase or decrease in the risk of contamination from a given cesspool based on its geographic position or the substrate upon which it is built. \n",
    "\n",
    "\n",
    "The factors presented below are calculated based on the groundwater model derived linkage between locations of human or ecological \"value\". The groundwater model is used to create flow-paths to derive the theoretical point of discharge (along the coast) of every OSDS unit. Ecological value was determined by marine science collaborators and human use value is based upon proxies of visitation and water recreation at coastal locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add visitation level from associated Flikr cell \n",
    "\n",
    " Coastline usage\n",
    "- Variability in human visitation along the coastline, the primary location of cesspool effluent discharge, was assessed indirectly following the peer-reviewed methodology of (Wood et al. 2013, also see InVEST recreation model documentation https://invest-userguide.readthedocs.io/en/latest/recreation.html), which has been previously proven to be a robust method of assessing user visitation.\n",
    "\n",
    "\n",
    "- Specifically, photo sharing location data was used as an indirect indicator of human visitation to coastal sites.\n",
    "\n",
    "\n",
    "-  Flikr an online photo sharing site offers an api service that allows users to query photos based on their geotagged locations. We developed a script (https://github.com/cshuler/Flikr_API_scraping) that assigned points every 250m along the coastline of Oahu, Maui and kauai and every 500 m on Big island, and then queried all the photos that fell within 250 m of each point. This data was then refined using the username of the photo owner and date to calculate the user-days for each point between 2010 and 2020. With a user day being a count of the number of the unique users visiting a given site on a given day, so that the count would not be biased by users who took large amounts of photos at a single location.\n",
    "\n",
    "\n",
    "- each osds unit was input into a groundwater model (Whittier and El-Kadi, 2009: 2014) and the path of effluent flow down the groundwater table to its discharge point on the coastline was calculated for every single osds in the state. Then the coastline usage data is geographically assigned to each osds unit, thereby allowing every unit to be scored higher or lower priority based on if it affects a coastline area with high or low usage by people\n",
    "\n",
    "\n",
    "Wood, SA, AD Guerry, JM Silver, M Lacayo. 2013. Using social media to quantify nature-based tourism and recreation. Scientific Reports 3: 2976. (https://www.nature.com/articles/srep02976) \n",
    "\n",
    "\n",
    "#### A note on the threshold value used to turn the number # of userdays into a 1-100 score\n",
    "\n",
    "##### 100 userdays seems to be a reasonable threshold value since it yields about 20 \"hotspot\" areas on Oahu meaning that if each visitor only went to one hot-spot then we would have about 2000 people uploading photos on a given day which is about 1% of Oahu's average daily visitor count (250K in Feb 2020)\n",
    "\n",
    "-   GreaterThan  1000 userdays is about 0.5% of total OSDS units\n",
    "-   GreaterThan  500 userdays is about 1.5% of total OSDS units\n",
    "-   GreaterThan  185 userdays is about 5% of total OSDS units\n",
    "-   GreaterThan  100 userdays is about 9% of total OSDS units\n",
    "-   GreaterThan  90 userdays is about 11% of total OSDS units\n",
    "-   GreaterThan  50 userdays is about 20% of total OSDS units\n",
    "\n",
    "(The above was determined using this code)  = \n",
    "\n",
    "```user_day_counts = OSDS_Master['UserDays'].value_counts()\n",
    "sum(list(user_day_counts[user_day_counts >50].index))/len(OSDS_Master)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this uses OSDS_FLIK_ID (the OSDS Uid to Flik ID key) from above\n",
    "\n",
    "paths_path = os.path.join(\"..\", \"Projected_data/Coast_Usage_Flikr/Processed\", 'Statewide_flikr_visitation.shp')\n",
    "columns_nams = [field.name for field in arcpy.ListFields(paths_path)]\n",
    "columns_nams.pop(1)  # remove stupid shape col\n",
    "temparr = arcpy.da.FeatureClassToNumPyArray(paths_path, columns_nams)\n",
    "tempdf =  pd.DataFrame(temparr)\n",
    "carelist = ['Flikr_ID', 'UserDays']  # cut to only wanted cols\n",
    "Flk_userTemp = tempdf[carelist].copy()\n",
    "\n",
    "# Do the merge with flikrcells\n",
    "usedays_merge = OSDS_FLIK_ID.merge(Flk_userTemp, on='Flikr_ID', how='left')\n",
    "carelist2 = ['Uid', 'UserDays']  # cut to only wanted cols\n",
    "\n",
    "USERDAYS = usedays_merge[carelist2]\n",
    "USERDAYS.to_csv(os.path.join(intermidiate_DataFramesPath, \"USERDAYS.csv\"))\n",
    "\n",
    "Hist_and_stats_on_DF(USERDAYS, \"UserDays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifeguard towers / swim beaches\n",
    "There are no lifeguard towers on Molokai. \n",
    "This shapefile was generated through searching websites that recommended beaches to swim, specifically the official HI tourism website: https://www.gohawaii.com/islands/molokai/things-to-do/beaches was used along with Visit Molokai https://visitmolokai.com/wp/molokai-beaches/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input datasets\n",
    "Flik_dots_path = os.path.join(\"..\", \"Projected_data/ImpactZones_Flik_ID/GeographicFlikr_data\", 'State_Gridded_Coast_WGS84_pts.shp')\n",
    "variable_pts_path = os.path.join(\"..\", \"Projected_data/Swim_beaches\", 'Molokai_Swim_Beaches.shp')\n",
    "\n",
    "# make silly arc layers\n",
    "arcpy.MakeFeatureLayer_management (Flik_dots_path, \"Flik_dots\")\n",
    "arcpy.MakeFeatureLayer_management (variable_pts_path, \"variable_pts\")\n",
    "\n",
    "# Select Flik cells(dots) by proximity to LG towers\n",
    "Selection = arcpy.SelectLayerByLocation_management(\"Flik_dots\", \"WITHIN_A_DISTANCE\", \"variable_pts\", \"550 meters\", \"NEW_SELECTION\",\"NOT_INVERT\")\n",
    "arcpy.CopyFeatures_management(Selection, os.path.join(tempspace, \"selected_variables.shp\"))\n",
    "\n",
    "# read data from shapefile \n",
    "extracted_points_path = os.path.join(tempspace, \"selected_variables.shp\")\n",
    "arcpy.TableToTable_conversion(extracted_points_path, tempspace, 'extracted_all.csv') # Create a rational file format\n",
    "# Create a rational PanDataframe\n",
    "Extracted_All = pd.read_csv(os.path.join(tempspace, 'extracted_all.csv'))\n",
    "\n",
    "# Cut out unneded columns and save final dataframe  (Note that the tempframe contains the actual flikr_ID of the guard station for each)\n",
    "want_cols = ['Uid', 'Swim_beach']\n",
    "tempframe = Extracted_All.merge(OSDS_FLIK_ID, on=\"Flikr_ID\", how=\"left\")\n",
    "tempframe[\"Swim_beach\"] = True\n",
    "SWIM_BEACHES = tempframe[want_cols]\n",
    "SWIM_BEACHES.to_csv(os.path.join(intermidiate_DataFramesPath, \"SWIM_BEACHES.csv\"))\n",
    "\n",
    "# print analysics on how OSDS file from top cell, matches up with results\n",
    "print(\"There are {} points draining to swim beaches\".format(len(SWIM_BEACHES)))\n",
    "print(\"There are {} OSDS shp points\".format(len(OSDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coastal Ecosystem impact valuations from Marine Biology Group\n",
    "Resource fish are coral reef fish species that make up a substantial proportion of non-commercial and commercial catch. Therefore this does not represent total fish biomass on the reef, but the subset of fish biomass that directly supports fishing and feeds local communities. Reef fish biomass has been shown to negatively correlate with effluent from OSDS in the Hawaiian Islands (Foo et al., 2021). \n",
    "  \n",
    "The Hawaii Monitoring and Reporting Collaborative (HIMARC) produced predicted maps of standing resource fish biomass and the theoretical recovery potential of resource fish biomass if effluent from on-site sewage disposal systems were eliminated (Donovan et al., 2020). \n",
    "\n",
    "A combined ranking of these 2 resource fish biomass datasets were derived for the cesspool conversion prioritization. Each of these datasets were summarized by median biomass within zones spanning 0 - 15 m depth corresponding to 1 km segments of shoreline.\n",
    "\n",
    "Shore based raster data was mapped onto 250 m grid polygons with 2 added columns (\"Coral_rank\", and \"RFish_rank\"). Both have integer values 1-4 where 1 = highest priority and 4 = lowest priority. Data in Pearl Harbor and Hawaii Kai Marina cells to NULL values since we have no reef data there.\n",
    "\n",
    "New datasets were generously generated for Molokai by the HIMARC group in 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_path = os.path.join(\"..\", \"Projected_data/Ecosystem\", 'Molokai_CoastGrid_250m_reefAtts_20250401.shp')\n",
    "columns_nams = [field.name for field in arcpy.ListFields(paths_path)]\n",
    "columns_nams.pop(1)  # remove stupid shape col\n",
    "temparr = arcpy.da.FeatureClassToNumPyArray(paths_path, columns_nams)\n",
    "tempdf =  pd.DataFrame(temparr)\n",
    "\n",
    "### This should not be needed with the new .._20250401.shp  But will need to modify when doing new update  ###\n",
    "# Do a replacement of the Flikr ID to include the island name \n",
    "#tempdf['Island'] = tempdf['Island'].str.replace('Hawaii','BI')\n",
    "#tempdf[\"new_flik_ID\"] = tempdf[\"Flikr_ID\"]+\"_\"+tempdf[\"Island\"]\n",
    "#tempdf[\"Flikr_ID\"] = tempdf[\"new_flik_ID\"]\n",
    "\n",
    "carelist = ['Flikr_ID', 'Coral_rank', \"RFish_rank\"]  # cut to only wanted cols\n",
    "Flk_EcoTemp = tempdf[carelist].copy()\n",
    "\n",
    "# Do the merge with flikrcells for corals\n",
    "coral_merge = OSDS_FLIK_ID.merge(Flk_EcoTemp, on='Flikr_ID', how='left')\n",
    "carelist2 = ['Uid', 'Coral_rank']  # cut to only wanted cols\n",
    "\n",
    "CORAL = coral_merge[carelist2]\n",
    "CORAL.to_csv(os.path.join(intermidiate_DataFramesPath, \"CORAL.csv\"))\n",
    "\n",
    "Hist_and_stats_on_DF(CORAL, \"Coral_rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the merge with flikrcells for fish    \n",
    "\n",
    "# Note coral_merge which came from Flk_EcoTemp exists from the cell above \n",
    "\n",
    "carelist3 = ['Uid', 'RFish_rank']  # cut to only wanted cols\n",
    "FISH = coral_merge[carelist3]\n",
    "FISH.to_csv(os.path.join(intermidiate_DataFramesPath, \"FISH.csv\"))\n",
    "\n",
    "Hist_and_stats_on_DF(FISH, \"RFish_rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean Circulation proxy\n",
    "##### Wave Power (in KW/m) Long-term Mean, 2000-2013 – Statewide\n",
    "\n",
    "Refs: Kappel, C.V., K.A. Selkoe, and Ocean Tipping Points (OTP). 2017. Wave Power Long-term Mean, 2000-2013 - Hawaii. Distributed by the Pacific Islands Ocean Observing System (PacIOOS). http://pacioos.org/metadata/hi_otp_all_wave_avg.html. Accessed 9-8-21.\n",
    "\n",
    "Wedding LM, Lecky J, Gove JM, Walecka HR, Donovan MK, et al. (2018) Advancing the integration of spatial data to map human and natural drivers on coral reefs. PLOS ONE 13(3): e0189792. https://doi.org/10.1371/journal.pone.0189792."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_path = os.path.join(\"..\", \"Projected_data/Coastal_Circulation\", 'Wave_power_KWm_Flikr_pts.shp')\n",
    "columns_nams = [field.name for field in arcpy.ListFields(paths_path)]\n",
    "columns_nams.pop(1)  # remove stupid shape col\n",
    "temparr = arcpy.da.FeatureClassToNumPyArray(paths_path, columns_nams)\n",
    "tempdf =  pd.DataFrame(temparr)\n",
    "\n",
    "carelist = ['Flikr_ID', 'Wave_pwr']  # cut to only wanted cols\n",
    "Flk_OcircTemp = tempdf[carelist].copy()\n",
    "\n",
    "# Do the merge with flikrcells for corals\n",
    "Ocirc_merge = OSDS_FLIK_ID.merge(Flk_OcircTemp, on='Flikr_ID', how='left')\n",
    "carelist2 = ['Uid', 'Wave_pwr']  # cut to only wanted cols\n",
    "\n",
    "WAVE_POWER = Ocirc_merge[carelist2]\n",
    "WAVE_POWER.to_csv(os.path.join(intermidiate_DataFramesPath, \"WAVE_POWER.csv\"))\n",
    "\n",
    "Hist_and_stats_on_DF(WAVE_POWER, \"Wave_pwr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidation into different areas and association of metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Census Track IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the key to join OSDS points to Census blocks \n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/Census\", '2010_Census_Tracts_Clean.shp')\n",
    "In_points = osds_path\n",
    "want_cols = ['Uid', 'Track_ID']\n",
    "Level = 'Tracts'\n",
    "\n",
    "TRACK_ID = Census_data_joining(In_points, In_polygons, want_cols, Save_meta=True, Level=Level)\n",
    "TRACK_ID.to_csv(os.path.join(intermidiate_DataFramesPath, \"TRACK_ID.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add census block group IDs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the key to join OSDS points to Census blockgroups\n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/Census\", '2010_Census_Block_Groups_Clean.shp')\n",
    "In_points = osds_path\n",
    "want_cols = ['Uid', 'BlockGp_ID']\n",
    "Level = 'Block_Groups'\n",
    "\n",
    "BLOCKGROUP_ID = Census_data_joining(In_points, In_polygons, want_cols, Save_meta=True, Level=Level)\n",
    "BLOCKGROUP_ID.to_csv(os.path.join(intermidiate_DataFramesPath, \"BLOCKGROUP_ID.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add census block IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the key to join OSDS points to Census blocks \n",
    "In_polygons = os.path.join(\"..\", \"Projected_data/Census\", '2010_Census_Blocks_Clean.shp')\n",
    "In_points = osds_path\n",
    "want_cols = ['Uid', 'BlockBk_ID']\n",
    "Level = 'Blocks'\n",
    "\n",
    "BLOCK_ID = Census_data_joining(In_points, In_polygons, want_cols, Save_meta=True, Level=Level)\n",
    "BLOCK_ID.to_csv(os.path.join(intermidiate_DataFramesPath, \"BLOCK_ID.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label if point is within a 2017 cesspool priority zone\n",
    "Should be none for Molokai as there dont seem to be any 2017 priority zones there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_polygons = os.path.join(\"..\", \"Projected_data/Old_2017_priority_areas\", 'Old_2017_priority_areas.shp')\n",
    "new_col_name = 'In_2017_CP_zone'\n",
    "In_points = osds_path\n",
    "\n",
    "IN_2017_CP = Find_points_inside_polys(In_points, In_polygons, new_col_name)\n",
    "IN_2017_CP.to_csv(os.path.join(intermidiate_DataFramesPath, \"IN_2017_CP.csv\"))\n",
    "\n",
    "print_stats_on_bool_layers(IN_2017_CP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge together all factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeframes_list = [BLOCK_ID, BLOCKGROUP_ID, TRACK_ID, OSDS_FLIK_ID, USERDAYS, SWIM_BEACHES,\n",
    "                    COASTLINE, MUN_WELLS, DOM_WELLS, STREAMS, SOILS_NRCS, RAINFALL,  SLOPE, OSDS_DENSITY, WETLANDS,\n",
    "                    DTOWATER, SLR_2030_yes, SLR_2050_yes, SLR_2100_yes, WellCZ_B_yes, WellCZ_C_yes, \n",
    "                    FISH, CORAL, IN_2017_CP, WAVE_POWER]  # ELEVATION  # PPL_PER_HOS,\n",
    "\n",
    "\n",
    " \n",
    "# Print out specs on the different lists\n",
    "for i in mergeframes_list: \n",
    "    print(f\"{i.columns}, length = {len(i)}\")   \n",
    "\n",
    "# Merge dataframes into master\n",
    "OSDS_Master = OSDS.copy()\n",
    "for i in mergeframes_list:\n",
    "    OSDS_Master = OSDS_Master.merge(i, on=\"Uid\", how=\"left\")\n",
    "\n",
    "# Some post processing \n",
    "OSDS_Master[\"dist2_Strm_Wtlnd_m\"] = OSDS_Master[[\"dist2_Streams_m\", \"dist2_Wetlands_m\"]].min(axis=1)\n",
    "OSDS_Master.drop_duplicates(subset=['Uid'], keep=\"first\", inplace=True)\n",
    "\n",
    "OSDS_Master.to_csv(os.path.join(\".\", \"Outputs/OSDS_Dots\", 'OSDS_MASTER_RiskFactors_v6.csv'), index =False)\n",
    "print(\"total length is {}\".format(len(OSDS_Master)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranks and weights \n",
    "\n",
    "Ingests the OSDS data with risk factors assigned and develops the **risk-factor scores** as well as applying **weights** to each parameter to develop a final **prioritization score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in initial thresholds off of csv\n",
    "thresholds =   pd.read_csv(os.path.join(\"..\", \"Projected_data\", 'Thresholds_and_Weights_v7_expWeight.csv'), index_col=0)\n",
    "numcols=['W1_base', 'W2_eco', 'W3_anthro', 'T1', 'T2', 'T3']\n",
    "thresholds[numcols] = thresholds[numcols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load datastets\n",
    "OSDS_Master = pd.read_csv(os.path.join(\"Outputs/OSDS_Dots\", 'OSDS_MASTER_RiskFactors_v6.csv'))\n",
    "OSDS_Master.drop_duplicates(subset=['Uid'], keep=\"first\", inplace=True)\n",
    "baseCols = ['X', 'Y', 'Island', 'TMK', 'Uid', 'In_2017_CP_zone',\n",
    "             'BlockBk_ID', 'BlockGp_ID', 'Track_ID', 'Flikr_ID']  # 'PepPerHos'\n",
    "priority_master = OSDS_Master[baseCols].copy()\n",
    "\n",
    "\n",
    "# Identify variables to consider and rank\n",
    "Rank_Cols = ['Soil_Suitability_Rank',  'SLR_Rank', 'WELLCZ_Rank', 'dist2_coast_m_Rank', \n",
    "             'dist2_Strm_Wtlnd_m_Rank', 'rainfall_in_Rank', 'dist2_MunWells_m_Rank', \n",
    "             'dist2_DomWells_m_Rank', 'Dep_to_Water_m_Rank', 'OSDS_Density_perAcre_Rank',\n",
    "             'Swim_beach_Rank', 'UserDays_Rank', 'Fish_Rank', 'Coral_Rank', 'Wave_pwr_Rank']  # , 'PepPerHos_Rank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with some naming issues\n",
    "OSDS_Master['nrcs_ksat_rep_slowwater'] = OSDS_Master['nrcs_ksat_rep'].copy()\n",
    "OSDS_Master['nrcs_ksat_rep_filtering'] = OSDS_Master['nrcs_ksat_rep'].copy()\n",
    "OSDS_Master['nrcs_ksat_rep_bottomseepage'] = OSDS_Master['nrcs_ksat_rep'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a consolidated soil suitability rank that takes into consideration: \n",
    "- Depth to Bedrock or Cemented Pan  100%       \n",
    "- Flooding or Ponding               100%  \n",
    "- Filtering Capacity (ft/d)         100% \n",
    "- Slow Water Movement (ft/d)        100% \n",
    "- Seepage From Bottom Layer (ft/d)  33%\n",
    "- Slope (percent)                   33% \n",
    "- Percent Rock Fragments > 3”       33%   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Soil suitability rank, with all 7 parameters equally weighted  ( high values = LEAST SUITABLE)\n",
    "\n",
    "### Note  Pandas telling you that its replace() downcasting behavior is changing in a future version.\n",
    "pd.set_option('future.no_silent_downcasting', False)  # Option 1 — Tell Pandas you’re fine with the old behavior:\n",
    "\n",
    "plist =  ['nrcs_ksat_rep_slowwater', 'nrcs_DtoBrock', 'nrcs_ksat_rep_filtering', \n",
    "          'nrcs_ksat_rep_bottomseepage', 'slope_deg', 'nrcs_rockFrag3in']\n",
    "\n",
    "# Prioritize thresholded ones\n",
    "framedic_Soil_temp = {}\n",
    "for idx, col in enumerate(plist):\n",
    "    Ty, T1, T2, T3 = read_3_thresholds(thresholds, col)\n",
    "    framedic_Soil_temp[\"Rank_{}\".format(col)] = cut_by_three_numeric_thresholds(OSDS_Master, col, Ty, T1, T2, T3)\n",
    "    \n",
    "    # Soil Type Flood Frequency \n",
    "FLOOD_FREQ = OSDS_Master[[\"Uid\", 'nrcs_FloodFreq']].copy()\n",
    "FLOOD_FREQ.loc[FLOOD_FREQ['nrcs_FloodFreq'] == \"None\", 'nrcs_FloodFreq_Rank'] = \"P4\"\n",
    "FLOOD_FREQ.loc[FLOOD_FREQ['nrcs_FloodFreq'] == \"Rare\", 'nrcs_FloodFreq_Rank'] = \"P3\"\n",
    "FLOOD_FREQ.loc[FLOOD_FREQ['nrcs_FloodFreq'] == \"Occasional\", 'nrcs_FloodFreq_Rank'] = \"P2\"\n",
    "FLOOD_FREQ.loc[FLOOD_FREQ['nrcs_FloodFreq'] == \"Frequent\", 'nrcs_FloodFreq_Rank'] = \"P1\"\n",
    "framedic_Soil_temp[\"Rank_nrcs_FloodFreq\"] = FLOOD_FREQ[[\"Uid\", 'nrcs_FloodFreq_Rank']]\n",
    "\n",
    "# Change out the P values with numeric values\n",
    "for key in framedic_Soil_temp.keys():\n",
    "    framedic_Soil_temp[key].replace(to_replace = \"P1\", value = 99, inplace = True)\n",
    "    framedic_Soil_temp[key].replace(to_replace = \"P2\", value = 66, inplace = True)\n",
    "    framedic_Soil_temp[key].replace(to_replace = \"P3\", value = 33, inplace = True)\n",
    "    framedic_Soil_temp[key].replace(to_replace = \"P4\", value = 1, inplace = True)\n",
    "    \n",
    "# Merge dataframes into master\n",
    "SOIL_MASTER = priority_master.copy()\n",
    "for i in framedic_Soil_temp.keys():\n",
    "    SOIL_MASTER = SOIL_MASTER.merge(framedic_Soil_temp[i], on=\"Uid\", how=\"left\")   \n",
    "    \n",
    "# This contains the internal weightings for the soil suitability rank\n",
    "Prioritization_vars_dic = {\n",
    "    'nrcs_ksat_rep_slowwater_Rank':1,\n",
    "    'nrcs_ksat_rep_bottomseepage_Rank':0.33,\n",
    "    'nrcs_ksat_rep_filtering_Rank':1,\n",
    "    'nrcs_DtoBrock_Rank':1,\n",
    "    'nrcs_FloodFreq_Rank':1,\n",
    "    'slope_deg_Rank':0.33,\n",
    "    'nrcs_rockFrag3in_Rank':0.33  }\n",
    "\n",
    "# Run the interlal weightings for the suitibility rank\n",
    "for var in Prioritization_vars_dic.keys():\n",
    "    SOIL_MASTER[var] = SOIL_MASTER[var]*Prioritization_vars_dic[var]\n",
    "\n",
    "SOIL_MASTER['Soil_Suitability_Rank'] = SOIL_MASTER[list(Prioritization_vars_dic.keys())].mean(axis=1)\n",
    "# Save the consolidated soil rank by itself\n",
    "SOIL_MASTER = SOIL_MASTER[['Uid', 'Soil_Suitability_Rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritization Function: Threshold then 1d-convective dispersive decay: 100 pt scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold then exponential decay\n",
    "def threshold_then_decay(OSDS_df, col_name, T1, ascending):\n",
    "    # reminder, 100 is bad/more impactful, 0 is good/least impactful\n",
    "    \n",
    "    return_frame = OSDS_df[[\"Uid\", col_name]].copy()\n",
    "    \n",
    "    scaler = return_frame[col_name].median()*.1   # this scale factor controls the shape of the exponential decay and essentially revises the value at which scores transition from high (90s and above) to lower\n",
    "    #print(\"for {} scaler is {}\".format(col_name, scaler))\n",
    "    \n",
    "    if ascending == True:\n",
    "        return_frame.loc[return_frame[col_name] < T1, '{}_Rank'.format(col_name)] = 100\n",
    "        return_frame.loc[return_frame[col_name] > T1, '{}_Rank'.format(col_name)] = 100*((T1+scaler)/(return_frame[col_name]+scaler))\n",
    "       \n",
    "        \n",
    "    if ascending == False:\n",
    "        return_frame.loc[return_frame[col_name] > T1, '{}_Rank'.format(col_name)] = 100\n",
    "        return_frame.loc[return_frame[col_name] < T1, '{}_Rank'.format(col_name)] = 100*(return_frame[col_name]+scaler)/(T1+scaler)\n",
    "\n",
    "    return_frame = return_frame[[\"Uid\",'{}_Rank'.format(col_name)]]\n",
    "    \n",
    "    return return_frame \n",
    "\n",
    "\n",
    "# a 0 to 100 scaler function\n",
    "def MY_minmaxscaler_dfCol(col, reverse=False):   \n",
    "    # Reverse True indicates that bigger data values   (e.g dist to coast) get smaller ranks meaning less impact \n",
    "    # Reverse False indicates that smaller data values (e.g OSDS_density)  get smaller ranks meaning less impact  \n",
    "    \n",
    "    mx = col.max()\n",
    "    mn = col.min()   \n",
    "    if reverse==False:\n",
    "        scaled_series = 100*(col-mn)/(mx-mn)       \n",
    "    if reverse==True:\n",
    "        scaled_series = 100*(col-mx)/(mn-mx)\n",
    "    \n",
    "    return scaled_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framedic_decay = {}\n",
    "\n",
    "# Loop over the non-soil parameters and add their ranks to the dic\n",
    "plist_ascending =  ['dist2_coast_m', 'dist2_Strm_Wtlnd_m', 'dist2_MunWells_m',\n",
    "                    'dist2_DomWells_m', 'Dep_to_Water_m', 'rainfall_in', 'Wave_pwr'] \n",
    "plist_decending =  ['OSDS_Density_perAcre', 'UserDays']\n",
    "\n",
    "for idx, col in enumerate(plist_ascending):\n",
    "    Ty, T1, T2, T3 = read_3_thresholds(thresholds, col)\n",
    "    framedic_decay[\"Rank_{}\".format(col)] = threshold_then_decay(OSDS_Master, col, T1, ascending=True)\n",
    "\n",
    "for idx, col in enumerate(plist_decending):\n",
    "    Ty, T1, T2, T3 = read_3_thresholds(thresholds, col)\n",
    "    framedic_decay[\"Rank_{}\".format(col)] = threshold_then_decay(OSDS_Master, col, T1, ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the non-loopable (thresholded) parameters to the framedic\n",
    "\n",
    "# Add the soil ranking\n",
    "framedic_decay['SOIL_MASTER'] = SOIL_MASTER\n",
    "\n",
    "# Sea Level Rise  'InSLR2030', 'InSLR2050',  'InSLR2100'\n",
    "SLR = OSDS_Master[[\"Uid\", 'InSLR2030', 'InSLR2050',  'InSLR2100']].copy()\n",
    "SLR['SLR_Rank'] = 0\n",
    "SLR.loc[SLR['InSLR2100'] == True, 'SLR_Rank'] = 33\n",
    "SLR.loc[SLR['InSLR2050'] == True, 'SLR_Rank'] = 66\n",
    "SLR.loc[SLR['InSLR2030'] == True, 'SLR_Rank'] = 100\n",
    "framedic_decay[\"Rank_SLR\"] = SLR[[\"Uid\", 'SLR_Rank']]\n",
    "\n",
    "# Well Capture Zones\n",
    "WELLCZs = OSDS_Master[[\"Uid\", 'In_WellCZ_B', 'In_WellCZ_C']].copy()\n",
    "WELLCZs['WELLCZ_Rank'] = 0\n",
    "WELLCZs.loc[WELLCZs['In_WellCZ_C'] == True, 'WELLCZ_Rank'] = 50\n",
    "WELLCZs.loc[WELLCZs['In_WellCZ_B'] == True, 'WELLCZ_Rank'] = 100\n",
    "framedic_decay[\"Rank_WELLCZ\"] = WELLCZs[[\"Uid\", 'WELLCZ_Rank']]\n",
    "\n",
    "# Swim beaches\n",
    "SWIMBEACH = OSDS_Master[[\"Uid\", 'Swim_beach']].copy()\n",
    "SWIMBEACH['Swim_beach_Rank'] = 0\n",
    "SWIMBEACH.loc[SWIMBEACH['Swim_beach'] == True, 'Swim_beach_Rank'] = 100\n",
    "framedic_decay[\"Rank_Swim_beach\"] = SWIMBEACH[[\"Uid\", 'Swim_beach_Rank']]\n",
    "\n",
    "# Fish   # Note that the original col was named RFish_rank with a small r the Fish_Rank with big R is the 0-100 score\n",
    "FISHY = OSDS_Master[[\"Uid\", 'RFish_rank']].copy()\n",
    "FISHY['Fish_Rank'] = 0\n",
    "FISHY.loc[FISHY['RFish_rank'] == 1, 'Fish_Rank'] = 100\n",
    "FISHY.loc[FISHY['RFish_rank'] == 2, 'Fish_Rank'] = 66\n",
    "FISHY.loc[FISHY['RFish_rank'] == 3, 'Fish_Rank'] = 33\n",
    "FISHY.loc[FISHY['RFish_rank'] == 4, 'Fish_Rank'] = 0\n",
    "framedic_decay[\"Rank_Fish\"] = FISHY[[\"Uid\", 'Fish_Rank']]\n",
    "\n",
    "# Coral    Note that the original col was named Coral_rank with a small r the Coral_Rank with big R is the 0-100 score\n",
    "CORALY = OSDS_Master[[\"Uid\", 'Coral_rank']].copy()\n",
    "CORALY['Coral_Rank'] = 0\n",
    "CORALY.loc[CORALY['Coral_rank'] == 1, 'Coral_Rank'] = 100\n",
    "CORALY.loc[CORALY['Coral_rank'] == 2, 'Coral_Rank'] = 66\n",
    "CORALY.loc[CORALY['Coral_rank'] == 3, 'Coral_Rank'] = 33\n",
    "CORALY.loc[CORALY['Coral_rank'] == 4, 'Coral_Rank'] = 0\n",
    "framedic_decay[\"Rank_Coral\"] = CORALY[[\"Uid\", 'Coral_Rank']]\n",
    "\n",
    "\n",
    "# Add number of people per household\n",
    "####Pep_hos = OSDS_Master[[\"Uid\", 'PepPerHos']].copy()\n",
    "####Pep_hos['PepPerHos_Rank'] = MY_minmaxscaler_dfCol(OSDS_Master['PepPerHos'], reverse=False)\n",
    "####framedic_decay[\"Rank_PepPerHos\"] = Pep_hos[[\"Uid\", 'PepPerHos_Rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes into master\n",
    "DECAY_PRIORITY_MASTER = priority_master.copy()\n",
    "for i in framedic_decay.keys():\n",
    "    DECAY_PRIORITY_MASTER = DECAY_PRIORITY_MASTER.merge(framedic_decay[i], on=\"Uid\", how=\"left\")\n",
    "\n",
    "# Create the single mean rank\n",
    "DECAY_PRIORITY_MASTER['Priority_mean'] = DECAY_PRIORITY_MASTER[Rank_Cols].mean(axis=1)\n",
    "\n",
    "print(len(DECAY_PRIORITY_MASTER))\n",
    "DECAY_PRIORITY_MASTER['Priority_mean'].hist(bins=50)\n",
    "\n",
    "#Print out a copy to use in the sensitivity testing notebook\n",
    "DECAY_PRIORITY_MASTER.to_csv(os.path.join(tempspace, \"DECAY_PRIORITY_MASTER.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between individual risk factor scores and the final priority score.  \n",
    "fig, ax = plt.subplots(3,5, figsize=(16, 9))\n",
    "for idx, col in enumerate(Rank_Cols):\n",
    "    ax = plt.subplot(3,5,idx+1)\n",
    "    x = DECAY_PRIORITY_MASTER['Priority_mean']\n",
    "    y = DECAY_PRIORITY_MASTER[col]\n",
    "    plt.scatter(x, y, marker=\".\", color='darkblue')\n",
    "    plt.ylabel(col); plt.xlabel('Final Priority Score')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Scenario each factor Weighted between 1-5 by expert panel \n",
    "\n",
    "This framework provides the ability to weigh the influence of each risk factor on the final priority score for each unit. Weighting can be done by multiplying the individual parameter score by a weight value, for example a weight value of two (2) can be used if it is desired for a parameter to be twice as important as those with a weighting value of one (1).\n",
    "\n",
    "\n",
    "Note, at one point I was scaling the final priority score into 0-100 space for comparisons between scenarios, but this might not be the best idea. used this syntax BASE_PRIORITY_MASTER['Weighted_Priority_mean'] = MY_minmaxscaler_dfCol(BASE_PRIORITY_MASTER['Weighted_Priority_mean'], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PRIORITY_MASTER = DECAY_PRIORITY_MASTER[baseCols].copy()  # Create base for weighting\n",
    "\n",
    "# Apply weights to each column\n",
    "for i in Rank_Cols: \n",
    "    col = i.replace(\"_Rank\", \"\")\n",
    "    weight = thresholds.loc[col]['W1_base']\n",
    "    \n",
    "    # Average the two methods and apply weights\n",
    "    BASE_PRIORITY_MASTER[i] = DECAY_PRIORITY_MASTER[i]*weight\n",
    "    \n",
    "    \n",
    "BASE_PRIORITY_MASTER['Weighted_Priority_mean'] = BASE_PRIORITY_MASTER[Rank_Cols].mean(axis=1)\n",
    "\n",
    "# prepare the final to be able to determine if more than 50% of the OSDS units are in a Priority zone (average wont count NaN as 0)\n",
    "BASE_PRIORITY_MASTER['In_2017_CP_zone'].fillna(False, inplace=True)\n",
    "\n",
    "# Plots \n",
    "BASE_PRIORITY_MASTER['Weighted_Priority_mean'].hist(bins=50)\n",
    "plt.title('Distribtution of scaled priority scores')\n",
    "\n",
    "BASE_PRIORITY_MASTER.to_csv(os.path.join(tempspace, \"BASE_PRIORITY_MASTER.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Individual OSDS units into Census Polygon groups:\n",
    "Different resolutions include: - 2010 Census Tracks, - 2010 Census Block Groups, - 2010 Census Blocks\n",
    "\n",
    "This assigns the average final priority score of the OSDS units that fall within each geographic polygon to the census unit. A parameter to exclude units that do not contain some minimum number of OSDS is available to avoid biasing the prioritization by areas with an insignificant number of OSDS units\n",
    "\n",
    "\n",
    "#####  Also delineate Priority categories (High, Med Low) Categories are defined primarily by our ranking¶\n",
    "- with top 10% highest priority scores as High\n",
    "- top 11% to 50% as Medium\n",
    "- Bottom 50% as low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the many CP units into tracks, remembering that the  Omit < is set at 25 \n",
    "UnitCol = 'Track_ID'\n",
    "Tracks_priority_frame_base = group_by_census_unit(BASE_PRIORITY_MASTER, UnitCol, omit_less_than=25)\n",
    "Tracks_priority_frame_base['OSDS_count'].hist(bins=len(Tracks_priority_frame_base['OSDS_count'].unique())+1)   # adding a +1 because the histogram looked weird with only 2 tracks in the frame\n",
    "plt.title(\"Histogram of the number of CP in each Census Track\"); plt.xlabel(\"Number of CP in track\"); plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut columns to desired ones \n",
    "WantCols = ['Track_ID', 'Final_Prioity_Score','Soil_Suitability_Rank', 'SLR_Rank', 'WELLCZ_Rank',\n",
    "       'dist2_coast_m_Rank', 'dist2_Strm_Wtlnd_m_Rank', 'rainfall_in_Rank',\n",
    "       'dist2_MunWells_m_Rank', 'dist2_DomWells_m_Rank', 'Dep_to_Water_m_Rank',\n",
    "       'OSDS_Density_perAcre_Rank', 'Swim_beach_Rank', 'UserDays_Rank', \"Fish_Rank\", \"Coral_Rank\", 'Wave_pwr_Rank',\n",
    "       'OSDS_count']   # will delete OSDS count once is in name col   #  'PepPerHos_Rank',\n",
    " \n",
    "compare_priority_score_Track = Tracks_priority_frame_base[WantCols]\n",
    "compare_priority_score_Track = compare_priority_score_Track.sort_values(\"Final_Prioity_Score\", ascending=False)\n",
    "\n",
    "\n",
    "# Get and set track names to the index\n",
    "trackpath = os.path.join(\"..\", \"Projected_data/Census/With_2017_priority\", '2010_Census_Tracts_Meta_w2017.shp') \n",
    "Track_meta = get_track_names(trackpath, \"Track_ID\")  # in the functions file\n",
    "compare_priority_score_Track = compare_priority_score_Track.merge(Track_meta, on='Track_ID', how='left')\n",
    "# Add on the # of CPs to the index \n",
    "compare_priority_score_Track['Name_ID'] = compare_priority_score_Track['Name_ID']+\": CPs=\"+compare_priority_score_Track['OSDS_count'].astype(str)\n",
    "compare_priority_score_Track = compare_priority_score_Track.set_index('Name_ID')                                  # Convert to pandas bliss\n",
    "del compare_priority_score_Track['Track_ID'];  del compare_priority_score_Track['OSDS_count']\n",
    "\n",
    "# make \n",
    "renamas = {'Final_Prioity_Score':'Final Prioity Score', \n",
    "            'Soil_Suitability_Rank':'Soil Suitability',\n",
    "            'SLR_Rank':'Sea Level Rise',\n",
    "            'WELLCZ_Rank':'Well Capture Zones',\n",
    "            'dist2_coast_m_Rank':'Dist. to Coast',\n",
    "            'dist2_Strm_Wtlnd_m_Rank':'Dist. to Strms/Wtlnds',\n",
    "            'rainfall_in_Rank':'Rainfall',\n",
    "            'dist2_MunWells_m_Rank':'Dist. to Muni. Wells',\n",
    "            'dist2_DomWells_m_Rank':'Dist. to Dom. Wells',\n",
    "            'Dep_to_Water_m_Rank':'Depth to GW',\n",
    "            'OSDS_Density_perAcre_Rank':'OSDS Density',\n",
    "            'Swim_beach_Rank':'Swim Beaches',\n",
    "            'UserDays_Rank':'Coastline Usage',\n",
    "            'Fish_Rank':'Reef Fishery Priority',\n",
    "            'Coral_Rank':'Coral Reef Priority', \n",
    "            'Wave_pwr_Rank': 'Wave Power'}    # , 'PepPerHos_Rank':'People per House'}\n",
    "\n",
    "compare_priority_score_Track.rename(columns=renamas, inplace=True)     # Rename to col that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_priority_score_Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out comparison heatmap \n",
    "\n",
    "Total_census_tracts = 3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 3))\n",
    "total_num = len(compare_priority_score_Track)\n",
    "#plt.title(\"Ranks for all {} census Tracks\".format(total_num), fontsize=18, y=1.05)\n",
    "plt.tick_params(axis='x', which='major', labelsize=10, labelbottom = True, bottom=True, top = True, labeltop=True)\n",
    "plt.xticks(rotation=70)\n",
    "bar_label = \"Priority score. Note total # of census units considered is {}\".format(len(Tracks_priority_frame_base['OSDS_count']))\n",
    "g= sns.heatmap(compare_priority_score_Track, annot=True,  cmap = 'YlOrBr', fmt=\".1f\", cbar_kws={'label': bar_label, \"shrink\": 0.5})\n",
    "plt.savefig(os.path.join(\".\", \"Outputs/Figures\", \"Tracks_Master_scores_chart.pdf\"), bbox_inches='tight')\n",
    "plt.savefig(os.path.join(\".\", \"Outputs/Figures\", \"Tracks_Master_scores_chart.png\"), bbox_inches='tight')\n",
    "\n",
    "print_info_for_islands(\"Molokai\", Tracks_priority_frame_base, Total_census_tracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick calculation of how many cesspools are in each catergory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSDS_Sum_Piv = pd.pivot_table(Tracks_priority_frame_base, index='Final_Cat_Ranking', aggfunc = 'sum')\n",
    "OSDS_Sum_Piv['OSDS_pct_in_rank'] = OSDS_Sum_Piv['OSDS_count']/len(OSDS)*100\n",
    "OSDS_Sum_Piv[['OSDS_count', 'OSDS_pct_in_rank']]\n",
    "# 62% in Low,  21% in Med, 16% in High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidation by block-groups\n",
    "Molokai is small and only has 2 tracts.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the many CP units into blockgroups  Rember the Omit < is set at 10 \n",
    "UnitCol = 'BlockGp_ID'\n",
    "BlockGp_priority_frame_base = group_by_census_unit(BASE_PRIORITY_MASTER, UnitCol, omit_less_than=10)\n",
    "BlockGp_priority_frame_base['OSDS_count'].hist(bins=len(BlockGp_priority_frame_base['OSDS_count'].unique())+1)   # adding a +1 because the histogram looked weird with only 2 blkgrps in the frame\n",
    "plt.title(\"Histogram of the number of CP in each Census BlockGp\"); plt.xlabel(\"Number of CP in BlockGp\"); plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut columns to desired ones \n",
    "WantCols = ['BlockGp_ID', 'Final_Prioity_Score','Soil_Suitability_Rank', 'SLR_Rank', 'WELLCZ_Rank',\n",
    "       'dist2_coast_m_Rank', 'dist2_Strm_Wtlnd_m_Rank', 'rainfall_in_Rank',\n",
    "       'dist2_MunWells_m_Rank', 'dist2_DomWells_m_Rank', 'Dep_to_Water_m_Rank',\n",
    "       'OSDS_Density_perAcre_Rank', 'Swim_beach_Rank', 'UserDays_Rank', \"Fish_Rank\", \"Coral_Rank\", 'Wave_pwr_Rank',\n",
    "       'OSDS_count']   # will delete OSDS count once is in name col   #  'PepPerHos_Rank',\n",
    " \n",
    "compare_priority_score_BlockGp = BlockGp_priority_frame_base[WantCols]\n",
    "compare_priority_score_BlockGp = compare_priority_score_BlockGp.sort_values(\"Final_Prioity_Score\", ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# Add on the # of CPs to the index \n",
    "compare_priority_score_BlockGp['BlockGp_ID'] = compare_priority_score_BlockGp['BlockGp_ID'].astype(str)+\": CPs=\"+compare_priority_score_BlockGp['OSDS_count'].astype(str)\n",
    "compare_priority_score_BlockGp = compare_priority_score_BlockGp.set_index('BlockGp_ID')                                  # Convert to pandas bliss\n",
    "del compare_priority_score_BlockGp['OSDS_count']\n",
    "\n",
    "# make \n",
    "renamas = {'Final_Prioity_Score':'Final Prioity Score', \n",
    "            'Soil_Suitability_Rank':'Soil Suitability',\n",
    "            'SLR_Rank':'Sea Level Rise',\n",
    "            'WELLCZ_Rank':'Well Capture Zones',\n",
    "            'dist2_coast_m_Rank':'Dist. to Coast',\n",
    "            'dist2_Strm_Wtlnd_m_Rank':'Dist. to Strms/Wtlnds',\n",
    "            'rainfall_in_Rank':'Rainfall',\n",
    "            'dist2_MunWells_m_Rank':'Dist. to Muni. Wells',\n",
    "            'dist2_DomWells_m_Rank':'Dist. to Dom. Wells',\n",
    "            'Dep_to_Water_m_Rank':'Depth to GW',\n",
    "            'OSDS_Density_perAcre_Rank':'OSDS Density',\n",
    "            'Swim_beach_Rank':'Swim Beaches',\n",
    "            'UserDays_Rank':'Coastline Usage',\n",
    "            'Fish_Rank':'Reef Fishery Priority',\n",
    "            'Coral_Rank':'Coral Reef Priority', \n",
    "            'Wave_pwr_Rank': 'Wave Power'}    # , 'PepPerHos_Rank':'People per House'}\n",
    "\n",
    "compare_priority_score_BlockGp.rename(columns=renamas, inplace=True)     # Rename to col that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_priority_score_BlockGp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out comparison heatmap \n",
    "\n",
    "Total_census_BlockGps = len(BLOCKGROUP_ID['BlockGp_ID'].unique())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 3))\n",
    "total_num = len(compare_priority_score_BlockGp)\n",
    "#plt.title(\"Ranks for all {} census Tracks\".format(total_num), fontsize=18, y=1.05)\n",
    "plt.tick_params(axis='x', which='major', labelsize=10, labelbottom = True, bottom=True, top = True, labeltop=True)\n",
    "plt.xticks(rotation=70)\n",
    "bar_label = \"Priority score. Note total # of census units considered is {}\".format(len(BlockGp_priority_frame_base['OSDS_count']))\n",
    "g= sns.heatmap(compare_priority_score_BlockGp, annot=True,  cmap = 'YlOrBr', fmt=\".1f\", cbar_kws={'label': bar_label, \"shrink\": 0.5})\n",
    "plt.savefig(os.path.join(\".\", \"Outputs/Figures\", \"BlockGp_Master_scores_chart.pdf\"), bbox_inches='tight')\n",
    "plt.savefig(os.path.join(\".\", \"Outputs/Figures\", \"BlockGp_Master_scores_chart.png\"), bbox_inches='tight')\n",
    "\n",
    "print_info_for_islands(\"Molokai\", BlockGp_priority_frame_base, Total_census_BlockGps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final rankings for block groups \n",
    "OSDS_Sum_Piv = pd.pivot_table(BlockGp_priority_frame_base, index='Final_Cat_Ranking', aggfunc = 'sum')\n",
    "OSDS_Sum_Piv['OSDS_pct_in_rank'] = OSDS_Sum_Piv['OSDS_count']/len(OSDS)*100\n",
    "OSDS_Sum_Piv[['OSDS_count', 'OSDS_pct_in_rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidation by blocks\n",
    "Molokai is small and only has 2 tracts.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the many CP units into blockgroups  Rember the Omit < is set at 5 \n",
    "UnitCol = 'BlockBk_ID'\n",
    "Block_priority_frame_base = group_by_census_unit(BASE_PRIORITY_MASTER, UnitCol, omit_less_than=5)\n",
    "Block_priority_frame_base['OSDS_count'].hist(bins=len(Block_priority_frame_base['OSDS_count'].unique()))   # adding a +1 because the histogram looked weird with only 2 blkgrps in the frame\n",
    "plt.title(\"Histogram of the number of CP in each Census Block\"); plt.xlabel(\"Number of CP in Block\"); plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut columns to desired ones \n",
    "WantCols = ['BlockBk_ID', 'Final_Prioity_Score','Soil_Suitability_Rank', 'SLR_Rank', 'WELLCZ_Rank',\n",
    "       'dist2_coast_m_Rank', 'dist2_Strm_Wtlnd_m_Rank', 'rainfall_in_Rank',\n",
    "       'dist2_MunWells_m_Rank', 'dist2_DomWells_m_Rank', 'Dep_to_Water_m_Rank',\n",
    "       'OSDS_Density_perAcre_Rank', 'Swim_beach_Rank', 'UserDays_Rank', \"Fish_Rank\", \"Coral_Rank\", 'Wave_pwr_Rank',\n",
    "       'OSDS_count']   # will delete OSDS count once is in name col   #  'PepPerHos_Rank',\n",
    " \n",
    "compare_priority_score_Block = Block_priority_frame_base[WantCols]\n",
    "compare_priority_score_Block = compare_priority_score_Block.sort_values(\"Final_Prioity_Score\", ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# Add on the # of CPs to the index \n",
    "compare_priority_score_Block['BlockBk_ID'] = compare_priority_score_Block['BlockBk_ID'].astype(str)+\": CPs=\"+compare_priority_score_Block['OSDS_count'].astype(str)\n",
    "compare_priority_score_Block = compare_priority_score_Block.set_index('BlockBk_ID')                                  # Convert to pandas bliss\n",
    "del compare_priority_score_Block['OSDS_count']\n",
    "\n",
    "# make \n",
    "renamas = {'Final_Prioity_Score':'Final Prioity Score', \n",
    "            'Soil_Suitability_Rank':'Soil Suitability',\n",
    "            'SLR_Rank':'Sea Level Rise',\n",
    "            'WELLCZ_Rank':'Well Capture Zones',\n",
    "            'dist2_coast_m_Rank':'Dist. to Coast',\n",
    "            'dist2_Strm_Wtlnd_m_Rank':'Dist. to Strms/Wtlnds',\n",
    "            'rainfall_in_Rank':'Rainfall',\n",
    "            'dist2_MunWells_m_Rank':'Dist. to Muni. Wells',\n",
    "            'dist2_DomWells_m_Rank':'Dist. to Dom. Wells',\n",
    "            'Dep_to_Water_m_Rank':'Depth to GW',\n",
    "            'OSDS_Density_perAcre_Rank':'OSDS Density',\n",
    "            'Swim_beach_Rank':'Swim Beaches',\n",
    "            'UserDays_Rank':'Coastline Usage',\n",
    "            'Fish_Rank':'Reef Fishery Priority',\n",
    "            'Coral_Rank':'Coral Reef Priority', \n",
    "            'Wave_pwr_Rank': 'Wave Power'}    # , 'PepPerHos_Rank':'People per House'}\n",
    "\n",
    "compare_priority_score_Block.rename(columns=renamas, inplace=True)     # Rename to col that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_priority_score_Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out comparison heatmap \n",
    "\n",
    "Total_census_Blocks = len(BLOCK_ID['BlockBk_ID'].unique())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "total_num = len(compare_priority_score_Block)\n",
    "#plt.title(\"Ranks for all {} census Tracks\".format(total_num), fontsize=18, y=1.05)\n",
    "plt.tick_params(axis='x', which='major', labelsize=10, labelbottom = True, bottom=True, top = True, labeltop=True)\n",
    "plt.xticks(rotation=70)\n",
    "bar_label = \"Priority score. Note total # of census units considered is {}\".format(len(Block_priority_frame_base['OSDS_count']))\n",
    "g= sns.heatmap(compare_priority_score_Block, annot=True,  cmap = 'YlOrBr', fmt=\".1f\", cbar_kws={'label': bar_label, \"shrink\": 0.5})\n",
    "plt.savefig(os.path.join(\".\", \"Outputs/Figures\", \"BlockGp_Master_scores_chart.pdf\"), bbox_inches='tight')\n",
    "plt.savefig(os.path.join(\".\", \"Outputs/Figures\", \"BlockGp_Master_scores_chart.png\"), bbox_inches='tight')\n",
    "\n",
    "print_info_for_islands(\"Molokai\", Block_priority_frame_base, Total_census_Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_priority_score_Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Census Area Shapefiles\n",
    "!! SUPER LONG RUNTIMES ON CELLS BELOW !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Shapefile, summarized by census Tracts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For tracks \n",
    "UnitCol=['Track_ID']\n",
    "# Cols for all of the below\n",
    "OutCols = ['Final_Prioity_Score', 'OSDS_count']  #  'PepPerHos', # 'Final_Prioity_Rank', \n",
    "all_cols = UnitCol + OutCols + Rank_Cols +[ 'Final_Cat_Ranking', 'Fin_Rank',]   # 'Final_Cat_Ranking'  comment out rank cols to just print simple shapefile  # 'In_2017_CP_zone',\n",
    "clean_cols = UnitCol + OutCols +['Final_Cat_Ranking', 'Fin_Rank']   #     'In_2017_CP_zone',\n",
    "\n",
    "# Group up the OSDS units into census areas \n",
    "Tracks_priority_frame = group_by_census_unit(BASE_PRIORITY_MASTER, UnitCol[0], omit_less_than=25)\n",
    "Tracks_priority_frame =   Tracks_priority_frame.merge(Track_meta, on='Track_ID', how='left')  # Add in the name ID from the Track_meta in above cell\n",
    "Tracks_priority_frame['Name_ID'] = Tracks_priority_frame['Name_ID']+\": CPs=\"+Tracks_priority_frame['OSDS_count'].astype(str)\n",
    "Tracks_priority_frame.set_index(\"Name_ID\", drop=True, inplace=True)\n",
    "\n",
    "# Save Full Shapefile \n",
    "Pivot_df = Tracks_priority_frame[all_cols]\n",
    "in_polygon_blocktrack = os.path.join(\"..\", \"Projected_data/Census\", '2010_census_tracts_Clean.shp')\n",
    "OutShpPath = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_Tracts_Full.shp')  \n",
    "make_census_unit_SHP(Pivot_df, all_cols, OutShpPath, in_polygon_blocktrack)\n",
    "Pivot_df.to_csv(os.path.join(\".\", \"Outputs/Census_aggregared_CSVs\", 'Final_Priority_Tracts_Full.csv'))\n",
    "\n",
    "# Save clean Shapefile \n",
    "Pivot_df = Tracks_priority_frame[clean_cols]\n",
    "in_polygon_blocktrack = os.path.join(\"..\", \"Projected_data/Census\", '2010_census_tracts_Clean.shp')\n",
    "OutShpPath = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_Tracts_Clean.shp')  \n",
    "make_census_unit_SHP(Pivot_df, clean_cols, OutShpPath, in_polygon_blocktrack)\n",
    "Pivot_df.to_csv(os.path.join(\".\", \"Outputs/Census_aggregared_CSVs\", 'Final_Priority_Tracts_Clean.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Shapefile, summarized by census Block_Groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnitCol=['BlockGp_ID']\n",
    "# Cols for all of the below\n",
    "OutCols = ['Final_Prioity_Score', 'OSDS_count']  #  'PepPerHos', # 'Final_Prioity_Rank', \n",
    "all_cols = UnitCol + OutCols + Rank_Cols +[ 'Final_Cat_Ranking', 'Fin_Rank',]   # 'Final_Cat_Ranking'  comment out rank cols to just print simple shapefile  # 'In_2017_CP_zone',\n",
    "clean_cols = UnitCol + OutCols +['Final_Cat_Ranking', 'Fin_Rank']   #  'In_2017_CP_zone',\n",
    "\n",
    "Block_groups_priority_frame = group_by_census_unit(BASE_PRIORITY_MASTER, UnitCol[0], omit_less_than=10)\n",
    "# get the name metadata from other metadata files\n",
    "Block_groups_meta = get_blkGrp_names(os.path.join(\"..\", \"Projected_data/Census/With_2017_priority\", '2010_Census_Block_Groups_Meta_w2017.shp'), 'BlockGp_ID')\n",
    "Block_groups_priority_frame =   Block_groups_priority_frame.merge(Block_groups_meta, on=UnitCol, how='left')  # Add in the name ID from the Track_meta in above cell\n",
    "Block_groups_priority_frame['Name_ID'] = Block_groups_priority_frame['Name_ID']+\": CPs=\"+Block_groups_priority_frame['OSDS_count'].astype(str)\n",
    "Block_groups_priority_frame.set_index(\"Name_ID\", drop=True, inplace=True)\n",
    "\n",
    "# Save Full Shapefile \n",
    "Pivot_df = Block_groups_priority_frame[all_cols]\n",
    "in_polygon_blocktrack = os.path.join(\"..\", \"Projected_data/Census\", '2010_Census_Block_Groups_Clean.shp')\n",
    "OutShpPath = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_BlockGrps_Full.shp') \n",
    "make_census_unit_SHP(Pivot_df, all_cols, OutShpPath, in_polygon_blocktrack)\n",
    "Pivot_df.to_csv(os.path.join(\".\", \"Outputs/Census_aggregared_CSVs\", 'Final_Priority_BlockGrps_Full.csv'))\n",
    "\n",
    "# Save clean Shapefile \n",
    "Pivot_df = Block_groups_priority_frame[clean_cols]\n",
    "in_polygon_blocktrack = os.path.join(\"..\", \"Projected_data/Census\", '2010_Census_Block_Groups_Clean.shp')\n",
    "OutShpPath = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_BlockGrps_Clean.shp') \n",
    "make_census_unit_SHP(Pivot_df, clean_cols, OutShpPath, in_polygon_blocktrack)\n",
    "Pivot_df.to_csv(os.path.join(\".\", \"Outputs/Census_aggregared_CSVs\", 'Final_Priority_BlockGrps_Clean.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Shapefile, summarized by census Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnitCol=['BlockBk_ID']\n",
    "# Cols for all of the below\n",
    "OutCols = ['Final_Prioity_Score', 'OSDS_count']  #  'PepPerHos', # 'Final_Prioity_Rank', \n",
    "all_cols = UnitCol + OutCols + Rank_Cols +['Final_Cat_Ranking', 'Fin_Rank',]   # 'Final_Cat_Ranking'  comment out rank cols to just print simple shapefile 'In_2017_CP_zone',\n",
    "clean_cols = UnitCol + OutCols +['Final_Cat_Ranking', 'Fin_Rank']   #  'In_2017_CP_zone',\n",
    "\n",
    "Blocks_priority_frame = group_by_census_unit(BASE_PRIORITY_MASTER, UnitCol[0], omit_less_than=5)\n",
    "\n",
    "# Save full shapefile \n",
    "Pivot_df = Blocks_priority_frame[all_cols]\n",
    "in_polygon_blocktrack = os.path.join(\"..\", \"Projected_data/Census\", '2010_Census_Blocks_Clean.shp')\n",
    "OutShpPath = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_Blocks_Full.shp') \n",
    "make_census_unit_SHP(Pivot_df, all_cols, OutShpPath, in_polygon_blocktrack)\n",
    "Pivot_df.to_csv(os.path.join(\".\", \"Outputs/Census_aggregared_CSVs\", 'Final_Priority_Blocks_Full.csv'))\n",
    "\n",
    "# Save clean Shapefile \n",
    "Pivot_df = Blocks_priority_frame[clean_cols]\n",
    "in_polygon_blocktrack = os.path.join(\"..\", \"Projected_data/Census\", '2010_Census_Blocks_Clean.shp')\n",
    "OutShpPath = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_Blocks_Clean.shp') \n",
    "make_census_unit_SHP(Pivot_df, clean_cols, OutShpPath, in_polygon_blocktrack)\n",
    "Pivot_df.to_csv(os.path.join(\".\", \"Outputs/Census_aggregared_CSVs\", 'Final_Priority_Blocks_Clean.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print final priority OSDS master  (cesspool dots) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final priority CP dots\n",
    "BASE_PRIORITY_MASTER.to_csv(os.path.join(os.path.abspath(\"Outputs/OSDS_Dots\"), \"Final_Priority_Master_CP.csv\"))\n",
    "\n",
    "\n",
    "# Convert CSV to a POINTS shapefile of the OSDS points \n",
    "XFieldName = 'X'                                 # THe x field longitude \n",
    "YFieldName = 'Y'                                 # THe y field latitude \n",
    "spatialRef = arcpy.SpatialReference(4326)        # THis is wgs84\n",
    "csvFilePath = os.path.join(os.path.abspath(\"Outputs/OSDS_Dots\"), \"Final_Priority_Master_CP.csv\")\n",
    "shpFilePath = os.path.abspath(\"Outputs/OSDS_Dots\")\n",
    "\n",
    "arcpy.MakeXYEventLayer_management(csvFilePath, XFieldName, YFieldName, 'Final_Priority_Master_CP', spatial_reference=spatialRef)\n",
    "arcpy.FeatureClassToShapefile_conversion('Final_Priority_Master_CP', shpFilePath)    # Shapefile name will be \"{}.shp\".format(var)\n",
    "\n",
    "# And print the raw riskfactors as a shapefile too\n",
    "csvFilePath = os.path.join(os.path.abspath(\"Outputs/OSDS_Dots\"), \"Final_Priority_Master_CP.csv\")\n",
    "shpFilePath = os.path.abspath(\"Outputs/OSDS_Dots\")\n",
    "\n",
    "\n",
    "arcpy.MakeXYEventLayer_management(csvFilePath, XFieldName, YFieldName, 'OSDS_MASTER_RiskFactors_v6', spatial_reference=spatialRef)\n",
    "arcpy.FeatureClassToShapefile_conversion('OSDS_MASTER_RiskFactors_v6', shpFilePath)    # Shapefile name will be \"{}.shp\".format(var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out OSDS dots shapefiles with census areas attributes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OSDS dots file with the synthesized TRACT area attributes \n",
    "In_polygons = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_Tracts_Clean.shp')  \n",
    "In_points = osds_path\n",
    "\n",
    "arcpy.SpatialJoin_analysis(In_points, In_polygons, os.path.join(\".\", \"Outputs/OSDS_Dots\", \"OSDS_dots_w_Tracts_Clean_atts.shp\"))\n",
    "\n",
    "\n",
    "# create OSDS dots file with the synthesized BLOCKGroups area attributes \n",
    "In_polygons = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_BlockGrps_Clean.shp')  \n",
    "In_points = osds_path\n",
    "\n",
    "arcpy.SpatialJoin_analysis(In_points, In_polygons, os.path.join(\".\", \"Outputs/OSDS_Dots\", \"OSDS_dots_w_BlkGrps_Clean_atts.shp\"))\n",
    "\n",
    "# create OSDS dots file with the synthesized BLOCKGroups area attributes \n",
    "In_polygons = os.path.join(\".\", \"Outputs\", \"Census_aggregared_SHPs\", 'Final_Priority_Blocks_Clean.shp')  \n",
    "In_points = osds_path\n",
    "\n",
    "arcpy.SpatialJoin_analysis(In_points, In_polygons, os.path.join(\".\", \"Outputs/OSDS_Dots\", \"OSDS_dots_w_Blocks_Clean_atts.shp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAQC on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mergeframes_list:\n",
    "    print(\"{}\".format(i.columns[-1]))\n",
    "    print(\"Number of points                   = {}\".format(len(i)))\n",
    "    print(\"Number of Uid that are null        = {}\".format(len(i[i[\"Uid\"].isnull()])))\n",
    "    print(\"Number of Uid that are zero        = {}\".format(len(i[i[\"Uid\"] == 0])))\n",
    "    print(\"Number of {} values that are null  = {}\".format(i.columns[-1], len(i[i[i.columns[-1]].isnull()])))\n",
    "    print(\"Number of {} values that are zero  = {}\".format(i.columns[-1], len(i[i[i.columns[-1]] == 0])))\n",
    "    print('\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DELETE  large files >100mb from offsite repo \n",
    "- Some of the files this notebook depends on are too large to be hosted by GitHub\n",
    "- One solution is to place these files into a cloud-based repositiry, in this case I use Digital Ocean spaces, and then just download them at the beginning of the work session\n",
    "- Once the analysis is complete USE THIS CELL to remove these files from the Github Repo so any updates dont include the large files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get rid of the Lidar files \n",
    "#delete_downloaded_files(LIDAR_file_urls, LIDAR_local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
